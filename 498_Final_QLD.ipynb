{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a50b31bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in training dataset (Airline sentiment tweet data)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "airline_df = pd.read_csv(\"hf://datasets/osanseviero/twitter-airline-sentiment/Tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45ba8d",
   "metadata": {},
   "source": [
    "### Dataset Description: Twitter Airline Sentiment\n",
    "The Twitter Airline Sentiment dataset originally came from Crowdflower's \"Data for Everyone\" library. It was originally created for a sentiment analysis, specifically in an attempt to find problems that customers noticed with each airline in the dataset.  Contributors were asked to classify the tweets as positive, neutral, or negative, and also asked to explain what issue was present in a tweet if it was negative (ex: \"delayed flight\"). Each tweet is also recorded with data like when it was created, the user who created the tweet, the sentiment label given to the tweet, and the airline the tweet is associated with.\n",
    "\n",
    "There are 14,640 tweets total in the dataset, and the tweets were scraped from the time period of February 16th, 2015 to February 24th, 2015. The average tweet length in the dataset is 19.12 words. \n",
    "\n",
    "We will be using this dataset to train our sentiment classification models.\n",
    "\n",
    "Tweet distribution per airline:\n",
    "United: 3822 || US Airways: 2913 || American: 2759 || Southwest: 2420 || Delta: 2222 || Virgin America: 504 \n",
    "\n",
    "Tweet distrubtion per sentiment:\n",
    "Negative: 9178 || Neutral: 3099 || Positive: 2363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2593182d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 15)\n",
      "2015-02-16 23:36:05 -0800\n",
      "2015-02-24 11:53:37 -0800\n",
      "{'Virgin America', 'Delta', 'Southwest', 'American', 'US Airways', 'United'}\n",
      "\n",
      "Tweet counts for airlines\n",
      "United            3822\n",
      "US Airways        2913\n",
      "American          2759\n",
      "Southwest         2420\n",
      "Delta             2222\n",
      "Virgin America     504\n",
      "Name: airline, dtype: int64\n",
      "\n",
      "Sentiment frequency\n",
      "negative    9178\n",
      "neutral     3099\n",
      "positive    2363\n",
      "Name: airline_sentiment, dtype: int64\n",
      "\n",
      "Average length of tweets: 19.122677595628414\n"
     ]
    }
   ],
   "source": [
    "# load twitter dataset into pandas \n",
    "from nltk.tokenize import TweetTokenizer \n",
    "\n",
    "print(airline_df.shape)\n",
    "print(min(airline_df['tweet_created'])) # Find earliest tweet for dataset\n",
    "print(max(airline_df['tweet_created'])) # Find earliest tweet for dataset\n",
    "\n",
    "print(set(airline_df['airline'])) # Find the unique airlines\n",
    "\n",
    "# Get airline tweet counts\n",
    "print()\n",
    "print(\"Tweet counts for airlines\")\n",
    "print(airline_df['airline'].value_counts())\n",
    "print()\n",
    "\n",
    "\n",
    "# Get airline sentiment counts\n",
    "print(\"Sentiment frequency\")\n",
    "print(airline_df['airline_sentiment'].value_counts())\n",
    "print() \n",
    "\n",
    "# Method for removing the \"@airline\" token from a tweet\n",
    "def _remove_airline_tok(tokens):\n",
    "    return tokens[1:] if tokens[0].startswith('@') else tokens   \n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "# Tokenize tweet text\n",
    "tweet_text = airline_df[\"text\"].values\n",
    "tweet_tokenized = [tokenizer.tokenize(tweet) for tweet in tweet_text]\n",
    "clean_tweets = [_remove_airline_tok(tokens) for tokens in tweet_tokenized]\n",
    "    \n",
    "# Find the average length of tweets\n",
    "len_arr = []\n",
    "for tweet in clean_tweets:\n",
    "    tweet_length = len(tweet)\n",
    "    len_arr.append(tweet_length)\n",
    "tweet_mean = np.mean(len_arr)\n",
    "print(\"Average length of tweets:\", tweet_mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "04379b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(airline_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8cc94e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful resource for multinomial regression: \n",
    "# https://machinelearningmastery.com/multinomial-logistic-regression-with-python/\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the airline data into training and test splits\n",
    "\n",
    "air_feats = airline_df[\"text\"]\n",
    "air_labels = airline_df[\"airline_sentiment\"]\n",
    "\n",
    "feat_train, feat_test, label_train, label_test = train_test_split(air_feats, air_labels, test_size = 0.8)\n",
    "\n",
    "# Tokenize the airline tweets in either split\n",
    "\n",
    "def _remove_airline_tok(tokens):\n",
    "    return tokens[1:] if tokens[0].startswith('@') else tokens    \n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "#Tokenizing training airline tweets\n",
    "train_tweets = feat_train.values\n",
    "train_tweets_tokenized = [tokenizer.tokenize(tweet) for tweet in train_tweets]\n",
    "train_tweets_clean = [_remove_airline_tok(tokens) for tokens in train_tweets_tokenized]\n",
    "\n",
    "#Tokenizing testing airline tweets\n",
    "test_tweets = feat_test.values\n",
    "test_tweets_tokenized = [tokenizer.tokenize(tweet) for tweet in test_tweets]\n",
    "test_tweets_clean = [_remove_airline_tok(tokens) for tokens in test_tweets_tokenized]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fe2c9f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading.\n"
     ]
    }
   ],
   "source": [
    "# loading word2vec vectors\n",
    "\n",
    "# Useful resource on word2vec \n",
    "# https://medium.com/swlh/sentiment-classification-using-word-embeddings-word2vec-aedf28fbb8ca\n",
    "\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v_file = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "w2v_file = os.path.expanduser(w2v_file)\n",
    "\n",
    "\n",
    "w2v_vectors = KeyedVectors.load_word2vec_format(w2v_file, binary=True)\n",
    "print('done loading.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "71fa135a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.95      0.86      7392\n",
      "     neutral       0.64      0.38      0.48      2447\n",
      "    positive       0.80      0.54      0.64      1873\n",
      "\n",
      "    accuracy                           0.77     11712\n",
      "   macro avg       0.74      0.62      0.66     11712\n",
      "weighted avg       0.76      0.77      0.74     11712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training multinomial regression using word2vec vectors\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Helpful guide: https://medium.com/swlh/sentiment-classification-using-word-embeddings-word2vec-aedf28fbb8ca\n",
    "\n",
    "\n",
    "\n",
    "# Function for creating the average word2vec vector for each tweet \n",
    "def tweet_to_avg_vector(tweet_tokens):\n",
    "    vectors = []\n",
    "    for word in tweet_tokens:\n",
    "        if word in w2v_vectors:\n",
    "            vectors.append(w2v_vectors[word])\n",
    "        else:\n",
    "            vectors.append(np.zeros(300))  # If word is not in word2Vec, use zero vector\n",
    "    return np.mean(vectors, axis=0)  # Average word vectors to represent the whole sentence \n",
    "\n",
    "# Convert training tweets to their average Word2Vec vector representations\n",
    "x_train_vectors = np.array([tweet_to_avg_vector(tweet) for tweet in train_tweets_clean])\n",
    "\n",
    "\n",
    "# Convert testing tweets to their average Word2Vec vector representations\n",
    "x_test_vectors = np.array([tweet_to_avg_vector(tweet) for tweet in test_tweets_clean])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train a multinomial logistic regression model (using word2vec vectors)\n",
    "mr = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "mr.fit(x_train_vectors, label_train)\n",
    "\n",
    "# Get predictions then evaluate performance on test set\n",
    "label_pred = mr.predict(x_test_vectors)\n",
    "print(classification_report(label_test, label_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "146db967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.96      0.85      7392\n",
      "     neutral       0.68      0.39      0.50      2447\n",
      "    positive       0.83      0.47      0.60      1873\n",
      "\n",
      "    accuracy                           0.76     11712\n",
      "   macro avg       0.76      0.61      0.65     11712\n",
      "weighted avg       0.76      0.76      0.74     11712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training multinomial regression using TF-IDF vectors \n",
    "\n",
    "# Get TF-IDF vectors after tokenizing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Convert tokenized tweets into strings that tf-idf vectorizer can actually use\n",
    "train_tweets_clean_joined = [' '.join(tokens) for tokens in train_tweets_clean]\n",
    "test_tweets_clean_joined = [' '.join(tokens) for tokens in test_tweets_clean]\n",
    "\n",
    "train_tweets_tfidf = vectorizer.fit_transform(train_tweets_clean_joined)\n",
    "test_tweets_tfidf = vectorizer.transform(test_tweets_clean_joined)\n",
    "\n",
    "\n",
    "# Train a multinomial logistic regression model\n",
    "mr = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "mr.fit(train_tweets_tfidf, label_train)\n",
    "\n",
    "# Get predictions then evaluate performance on test set\n",
    "label_pred = mr.predict(test_tweets_tfidf)\n",
    "print(classification_report(label_test, label_pred))\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7c46d878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                precision    recall  f1-score   support\\n\\nword2vec:\\n    accuracy                           0.76     11712\\n   macro avg       0.75      0.61      0.65     11712\\nweighted avg       0.76      0.76      0.74     11712\\n\\n\\ntf-idf:\\n    accuracy                           0.75     11712\\n   macro avg       0.77      0.59      0.64     11712\\nweighted avg       0.76      0.75      0.72     11712\\n'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "              \n",
    "\n",
    "word2vec:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "    negative       0.77      0.96      0.85      7333\n",
    "     neutral       0.67      0.36      0.47      2471\n",
    "    positive       0.82      0.50      0.62      1908\n",
    "\n",
    "    accuracy                           0.76     11712\n",
    "   macro avg       0.75      0.61      0.65     11712\n",
    "weighted avg       0.75      0.76      0.74     11712\n",
    "\n",
    "\n",
    "tf-idf:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "    negative       0.75      0.97      0.84      7333\n",
    "     neutral       0.68      0.35      0.46      2471\n",
    "    positive       0.84      0.42      0.56      1908\n",
    "\n",
    "    accuracy                           0.75     11712\n",
    "   macro avg       0.76      0.58      0.62     11712\n",
    "weighted avg       0.75      0.75      0.72     11712\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867aa7e7",
   "metadata": {},
   "source": [
    "### TF-idf or word2vec (airline data)?\n",
    "\n",
    "#### Word2vec\n",
    "Per class performance of the multinomial regression model trained on word2vec:\n",
    "Negative: Precision = 0.77 || Recall = 0.96 || F1-score = 0.85\n",
    "Neutral: Precision = 0.67 || Recall = 0.36 || F1-score = 0.47\n",
    "Positive: Precision = 0.82 || Recall = 0.50 || F1-score = 0.62\n",
    "\n",
    "The airline dataset is imbalanced with a lot of negative tweets, and the model seems to have gotten better at classifying negative tweets than neutral or positive ones. Looking at the recall scores specifically, we can see that the model correctly classied 96% of the negative tweets, but much lower for the neutral (36%) and positive (50%) tweets. This is reflected in the per-class F1 scores as well, with the negative class F1 score being at a solid 0.85, while the neutral (0.47) and positive (0.62) F1 scores are much lower.\n",
    "\n",
    "The average accuracy of the multinomial regression model trained on word2vec vectors is 0.76. The macro-averaged F1-score is 0.65, while the micro-averaged F1-score is 0.74. This makes sense, as the higher number of negative tweets and the model's superior performance on those negative tweets would bring the micro-F1 up, while the macro-F1 reflects that it did worse on classifying neutral and positive tweets.\n",
    "\n",
    "\n",
    "#### TF-idf\n",
    "\n",
    "Per class performance of the multinomial regression model trained on TF-idf vectors:\n",
    "Negative: Precision = 0.75 || Recall = 0.97 || F1-score = 0.84\n",
    "Neutral: Precision = 0.68 || Recall = 0.35 || F1-score = 0.46\n",
    "Positive: Precision = 0.84 || Recall = 0.42 || F1-score = 0.57\n",
    "\n",
    "Again, the imbalance in the distribution of sentiments in the tweets is visible here. The model performed better on negative tweets (F1-score: 0.84) than it did on neutral (F1-score: 0.46) or positive tweets (F1-score: 0.57). Additionally, we can see the same pattern where the micro-F1 score, which gives equal weight to all instances, is biased by the high number of negative tweets and sits at 0.72 while the macro-average, which gives equal weight to all classes, is lower at 0.62.\n",
    "\n",
    "Comparing the averages between the two models, it seems that word2vec performs slightly better but it's a small difference and word2vec takes much longer to run, so I'd just stick to using the TF-idf vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0f8ddcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the main Twitter corpus\n",
    "twitter_df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding =\"latin-1\", \n",
    "                         names=[\"sentiment\", \"id\", \"date\", \"flag\", \"user\", \"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7ff53d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id                          date      flag  \\\n",
       "0                0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1                0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2                0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3                0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4                0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...            ...         ...                           ...       ...   \n",
       "1599995          4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996          4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997          4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998          4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999          4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(twitter_df.shape)\n",
    "display(twitter_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b626b",
   "metadata": {},
   "source": [
    "### Dataset Description: Twitter Sentiment140\n",
    "The Sentiment140 dataset is a dataset of Tweets that was collected as part of a research paper which can be found at this link: (https://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf).\n",
    "\n",
    "The original use of the dataset was test how machine learning models would perform when classifying the sentiment of Tweets when trained on data including emoticons like \":)\". Rather than hand-labeling data, they used the emoticons as a noisy label for the sentiment of each tweet. Tweets with emoticons like \":)\" were labeled as containing positive sentiment, while tweets with emoticons like \":(\" were labeled as negative. \n",
    "\n",
    "The Tweets were gathered through various queries using Twitter's API, and tweets containing both positive and negative emoticons were removed from the dataset. Each tweet is also recorded with data like when it was created, the user who created the tweet, the sentiment label given to the tweet, and the query used to find the tweet, if any.\n",
    "\n",
    "There are 1,600,000 Tweets in the dataset, and there are tweets from 659,775 different users. The tweets are taken from the time period of April 17th, 2009 to May 27th, 2009. The distribution of sentiments is even, with 800,000 positive tweets and 800,000 negative tweets. The average length of tweets in the corpus is 14.85 words.\n",
    "\n",
    "We will be using this dataset as the corpus we're analyzing.\n",
    "\n",
    "##### Note: Realized Kaggle tricked me because the Sentiment140 dataset page had a note that said \"target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\" but there are actually only positive / negative labels in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "12de8f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "\n",
    "# Create function to convert numerical sentiment labels into text\n",
    "def map_sentiment(value):\n",
    "    if value == 0:\n",
    "        return 'negative'\n",
    "    elif value == 2:\n",
    "        return 'neutral'\n",
    "    elif value == 4:\n",
    "        return 'positive'\n",
    "\n",
    "twitter_df[\"sentiment_text\"] = twitter_df[\"sentiment\"].apply(map_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bf1ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(twitter_df[\"sentiment\"].head(-40))\n",
    "# print(twitter_df[\"sentiment_text\"].head(-40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b8221973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 17 20:30:31 PDT 2009\n",
      "Wed May 27 07:27:38 PDT 2009\n",
      "Unique user count: 659775\n",
      "Sentiment frequency\n",
      "0    800000\n",
      "4    800000\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "Average length of tweets: 14.852441875\n"
     ]
    }
   ],
   "source": [
    "print(min(twitter_df['date'])) # Find earliest tweet for dataset\n",
    "print(max(twitter_df['date'])) # Find earliest tweet for dataset\n",
    "\n",
    "# Find number of different users in the dataset\n",
    "unique_users = set(twitter_df[\"user\"])\n",
    "unique_user_count = len(unique_users)\n",
    "print(\"Unique user count:\", unique_user_count)\n",
    "\n",
    "# Get frequency of each sentiment\n",
    "print(\"Sentiment frequency\")\n",
    "print(twitter_df['sentiment'].value_counts())\n",
    "print() \n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "# Tokenize tweet text\n",
    "# also removing the \"@person\" token from a tweet; cleans up any tweets that are replies to other users\n",
    "tweet_text = twitter_df[\"text\"].values\n",
    "tweet_tokenized = [tokenizer.tokenize(tweet) for tweet in tweet_text]\n",
    "clean_tweets = [_remove_airline_tok(tokens) for tokens in tweet_tokenized]\n",
    "    \n",
    "# Find the average length of tweets\n",
    "len_arr = []\n",
    "for tweet in clean_tweets:\n",
    "    tweet_length = len(tweet)\n",
    "    len_arr.append(tweet_length)\n",
    "tweet_mean = np.mean(len_arr)\n",
    "print(\"Average length of tweets:\", tweet_mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b26a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER entity types:\n",
    "# PERSON:      People, including fictional.\n",
    "# NORP:        Nationalities or religious or political groups.\n",
    "# FAC:         Buildings, airports, highways, bridges, etc.\n",
    "# ORG:         Companies, agencies, institutions, etc.\n",
    "# GPE:         Countries, cities, states.\n",
    "# LOC:         Non-GPE locations, mountain ranges, bodies of water.\n",
    "# PRODUCT:     Objects, vehicles, foods, etc. (Not services.)\n",
    "# EVENT:       Named hurricanes, battles, wars, sports events, etc.\n",
    "# WORK_OF_ART: Titles of books, songs, etc.\n",
    "# LAW:         Named documents made into laws.\n",
    "# LANGUAGE:    Any named language.\n",
    "# DATE:        Absolute or relative dates or periods.\n",
    "# TIME:        Times smaller than a day.\n",
    "# PERCENT:     Percentage, including ”%“.\n",
    "# MONEY:       Monetary values, including unit.\n",
    "# QUANTITY:    Measurements, as of weight or distance.\n",
    "# ORDINAL:     “first”, “second”, etc.\n",
    "# CARDINAL:    Numerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc38a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for NER!\n",
    "\n",
    "# Load spacy models and using for NER\n",
    "import spacy\n",
    "\n",
    "# load a spacy model trained on web text\n",
    "nlp_web = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc_web = twitter_df['text'].apply(nlp_web) # Apply nlp web to each row in the ['document'] series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16f86431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [(@switchfoot http://twitpic.com/2y1zl -, PERS...\n",
       "1                                            [(School, ORG)]\n",
       "2                                           [(50%, PERCENT)]\n",
       "3                                                         []\n",
       "4                                                         []\n",
       "                                 ...                        \n",
       "1599995                                                   []\n",
       "1599996                                     [(Walt, PERSON)]\n",
       "1599997                                                   []\n",
       "1599998      [(38th, ORDINAL), (Tupac Amaru Shakur, PERSON)]\n",
       "1599999                                      [(#, CARDINAL)]\n",
       "Name: text, Length: 1600000, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_entities(doc): # Function for getting the entity text + entity labels for each doc \n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "entities = doc_web.apply(extract_entities)\n",
    "display(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "059dd639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of entities per tweet: 0.934395625\n",
      "PERSON         344579.0\n",
      "DATE           291742.0\n",
      "ORG            305084.0\n",
      "PERCENT          2850.0\n",
      "ORDINAL         26742.0\n",
      "CARDINAL       158404.0\n",
      "LOC             11457.0\n",
      "TIME           114966.0\n",
      "GPE            138184.0\n",
      "NORP            36439.0\n",
      "QUANTITY         5938.0\n",
      "WORK_OF_ART     12484.0\n",
      "FAC              8745.0\n",
      "MONEY           13660.0\n",
      "PRODUCT         16398.0\n",
      "LANGUAGE         3032.0\n",
      "EVENT            2842.0\n",
      "LAW              1487.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "entity_sum = 0\n",
    "for doc in entities:\n",
    "    entity_count = len(doc)\n",
    "    entity_sum += entity_count\n",
    "    \n",
    "entity_avg = entity_sum / len(entities)\n",
    "print(\"Average number of entities per tweet:\", entity_avg)\n",
    "\n",
    "\n",
    "# Now find avg number of each entity type\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Function to count entity types in a single document\n",
    "def count_entity_types(doc):\n",
    "    entity_types = [ent.label_ for ent in doc.ents]\n",
    "    return Counter(entity_types)\n",
    "\n",
    "entity_counts = doc_web.apply(count_entity_types)\n",
    "\n",
    "# Convert to dataframe to make summarizing easier\n",
    "entity_counts_df = pd.DataFrame(list(entity_counts))\n",
    "\n",
    "# Find the sum of instances for each entity type\n",
    "average_entity_counts = entity_counts_df.sum()\n",
    "\n",
    "print(average_entity_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9ba57b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON: \n",
      "  - Twitter: 3712 instances\n",
      "  - Lol: 1917 instances\n",
      "  - @ddlovato: 1636 instances\n",
      "DATE: \n",
      "  - today: 57597 instances\n",
      "  - tomorrow: 27349 instances\n",
      "  - yesterday: 8535 instances\n",
      "ORG: \n",
      "  - LOL: 8055 instances\n",
      "  - iPod: 1262 instances\n",
      "  - LOVE: 1146 instances\n",
      "PERCENT: \n",
      "  - 100%: 587 instances\n",
      "  - 50%: 134 instances\n",
      "  - 10%: 124 instances\n",
      "ORDINAL: \n",
      "  - first: 12105 instances\n",
      "  - second: 2461 instances\n",
      "  - 1st: 1539 instances\n",
      "CARDINAL: \n",
      "  - 2: 25610 instances\n",
      "  - one: 14757 instances\n",
      "  - 4: 9613 instances\n",
      "LOC: \n",
      "  - NYC: 1093 instances\n",
      "  - Europe: 512 instances\n",
      "  - new moon: 299 instances\n",
      "TIME: \n",
      "  - tonight: 23124 instances\n",
      "  - last night: 9733 instances\n",
      "  - morning: 8907 instances\n",
      "GPE: \n",
      "  - London: 1901 instances\n",
      "  - LA: 1891 instances\n",
      "  - UK: 1883 instances\n",
      "NORP: \n",
      "  - french: 1109 instances\n",
      "  - english: 775 instances\n",
      "  - Congrats: 630 instances\n",
      "QUANTITY: \n",
      "  - a ton: 273 instances\n",
      "  - a mile: 73 instances\n",
      "  - 100 degrees: 57 instances\n",
      "WORK_OF_ART: \n",
      "  - Love: 2551 instances\n",
      "  - Star Trek: 355 instances\n",
      "  - LoL: 225 instances\n",
      "FAC: \n",
      "  - Disneyland: 205 instances\n",
      "  - metro: 158 instances\n",
      "  - True Blood: 152 instances\n",
      "MONEY: \n",
      "  - #fb: 481 instances\n",
      "  - 1: 395 instances\n",
      "  - 2: 329 instances\n",
      "PRODUCT: \n",
      "  - Twitter: 3008 instances\n",
      "  - â¥: 292 instances\n",
      "  - Magic: 135 instances\n",
      "LANGUAGE: \n",
      "  - English: 1181 instances\n",
      "  - english: 1021 instances\n",
      "  - French: 104 instances\n",
      "EVENT: \n",
      "  - Wimbledon: 125 instances\n",
      "  - the French Open: 77 instances\n",
      "  - @gfalcone601: 55 instances\n",
      "LAW: \n",
      "  - TONS: 35 instances\n",
      "  - Thnx 4: 14 instances\n",
      "  - ï¿½: 12 instances\n"
     ]
    }
   ],
   "source": [
    "entity_labels = ['PERSON', 'DATE', 'ORG', 'PERCENT', 'ORDINAL', 'CARDINAL', 'LOC', 'TIME', 'GPE', 'NORP', 'QUANTITY',\n",
    "                 'WORK_OF_ART', 'FAC', 'MONEY', 'PRODUCT', 'LANGUAGE', 'EVENT', 'LAW']\n",
    "\n",
    "# Dictionary to store counts for each type\n",
    "entity_spans = {label: Counter() for label in entity_labels}\n",
    "\n",
    "# Iterate over each document then count spans\n",
    "for doc in doc_web:\n",
    "    for ent in doc.ents:\n",
    "        entity_spans[ent.label_].update([ent.text]) \n",
    "\n",
    "\n",
    "# Helpful guide on using items() to iterate through keys+values in a dict: \n",
    "# https://www.geeksforgeeks.org/iterate-over-a-dictionary-in-python/\n",
    "# Finding most common item in a list:\n",
    "# https://stackoverflow.com/questions/3594514/how-to-find-most-common-elements-of-a-list\n",
    "most_common_ents = {}\n",
    "for entity, spans in entity_spans.items():\n",
    "    if spans:  # Confirm there are named entities in the doc\n",
    "        \n",
    "        #Only shows the top 1 most common span\n",
    "        #most_common_span, count = spans.most_common(1)[0]  # Get the most common span and its count\n",
    "        #most_common_ents[entity] = (most_common_span, count)  # Add it to the result dictionary\n",
    "        \n",
    "        # Show top 3 common spans\n",
    "        most_common_spans = spans.most_common(3)  # Get the top 3 most common spans and their counts\n",
    "        most_common_ents[entity] = most_common_spans  # Add them to the result dictionary\n",
    "    \n",
    "    \n",
    "# Print the most common span for each entity type\n",
    "# for entity, (span, count) in most_common_ents.items():\n",
    "#     print(entity + \": Most common span = \" + span + \" (\"+ str(count) + \" instances)\")\n",
    "    \n",
    "# Print the top 3 most common spans for each entity type\n",
    "for entity, common_spans in most_common_ents.items():\n",
    "    print(entity + \": \")\n",
    "    for span, count in common_spans:\n",
    "        print(f\"  - {span}: {count} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed77b0e",
   "metadata": {},
   "source": [
    "### Baseline entity analysis across all tweets \n",
    "There are 0.93 entities in each tweet, on average. The most common type of entity is \"PERSON\" (344,579 tweets), followed by \"ORG\" (305,084 tweets), and \"DATE\" (291,742) tweets. The least common entity types are \"LAW\" (1,487 tweets), \"EVENT\" (2,842 teets), and \"PERCENT\" (2,850 tweets).\n",
    "\n",
    "The most common spans per entity type are as follows:\n",
    "- PERSON: \"Twitter\", \"Lol\", and \"@ddlovato\"\n",
    "- DATE: \"Today\", \"tomorrow\", and \"yesterday\"\n",
    "- ORG: \"LOL\", \"iPod\", and \"LOVE\"\n",
    "- PERCENT: \"100%\", \"50%\", and \"10%\"\n",
    "- ORDINAL: \"first\", \"second\", and \"1st\"\n",
    "- CARDINAL: \"2\", \"one\", and \"4\"\n",
    "- LOC: \"NYC\", \"Europe\", and \"new moon\"\n",
    "- TIME: \"tonight\", \"last night\", and \"morning\"\n",
    "- GPE: \"London\", \"LA\", and \"UK\"\n",
    "- NORP: \"french\", \"english\", and \"Congrats\"\n",
    "- QUANTITY: \"a ton\", \"a mile\", and \"100 degrees\"\n",
    "- WORK_OF_ART: \"Love\", \"Star Trek\" and \"LoL\"\n",
    "- FAC: \"Disneyland\", \"metro\", and \"True Blood\"\n",
    "- MONEY: \"#fb\", \"1\", and \"2\"\n",
    "- PRODUCT: \"Twitter\", \"â¥\", and \"Magic\"\n",
    "- LANGUAGE: \"English\", \"english\", \"French\"\n",
    "- EVENT: \"Wimbledon\", \"the French Open\", \"@gfalcone601\"\n",
    "- LAW: \"TONS\", \"Thnx 4\", \"ï¿½\"\n",
    "\n",
    "We can see that there are some interesting parts of these results, like the NER algorithm treating \"English\" and \"english\" as different languages, or treating \"English\" and \"english\" as different languages, though we can't just remove all capitalization as I'm sure that plays a role in helping the NER model to decide what is or isn't an entity. We can definitely see that these entites were pulled from the past. Entitiesl like \"iPod\" and \"@ddlovato\" (Demi Lovato) are definitely less culturally relevant now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5b1e605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tduon\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\tduon\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.88      0.69    800000\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       0.85      0.16      0.27    800000\n",
      "\n",
      "    accuracy                           0.52   1600000\n",
      "   macro avg       0.47      0.35      0.32   1600000\n",
      "weighted avg       0.71      0.52      0.48   1600000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tduon\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Get a random sample of 1000 rows to test code without taking forever\n",
    "# corpus_sample = twitter_df.sample(n=1000) \n",
    "# For full dataset, use \"twitter_df\" instead to replace \"corpus_sample\"\n",
    "\n",
    "####\n",
    "# Tokenize tweet text\n",
    "tweet_text = twitter_df[\"text\"].values  \n",
    "# tweet_text = corpus_sample[\"text\"].values\n",
    "tweet_tokenized = [tokenizer.tokenize(tweet) for tweet in tweet_text]\n",
    "corpus_tweets_clean = [_remove_airline_tok(tokens) for tokens in tweet_tokenized]\n",
    "\n",
    "# Convert tokenized tweets into strings that tf-idf vectorizer can actually use\n",
    "corpus_tweets_clean_joined = [' '.join(tokens) for tokens in corpus_tweets_clean]\n",
    "####\n",
    "\n",
    "# train_tweets_tfidf = vectorizer.fit_transform(train_tweets_clean_joined)\n",
    "corpus_tweets_tfidf = vectorizer.transform(corpus_tweets_clean_joined)\n",
    "\n",
    "\n",
    "# Get predictions then evaluate performance on test set\n",
    "corpus_label_pred = mr.predict(corpus_tweets_tfidf)\n",
    "print(classification_report(twitter_df[\"sentiment_text\"], corpus_label_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3584dc",
   "metadata": {},
   "source": [
    "### Uh oh, we need new training data\n",
    "Realized here that the training data having a \"neutral\" label but the corpus itself not having \"neutral\" tweets in it would be a problem. The trained model performed horribly (0.52 average accuracy, 0.31 macro f1 score, 0.47 micro f1 score)! This would also mean it's a binary classification task, so I should switch to logistic regression. No more multinomial regression situation!\n",
    "\n",
    "### Dataset Description: large-twitter-tweets-sentiment\n",
    "Pulled from Huggingface (link: https://huggingface.co/datasets/gxb912/large-twitter-tweets-sentiment), there are 224,994 tweets in it total, with 179,995 tweets in the training split and 44,999 tweets in the testing split. The dataset didn't include any data aside from the text and sentiment label of each tweet. There was no other information on the source of the dataset aside from the fact that it was annotated specifically for sentiment analysis.\n",
    "\n",
    "In the training split, there are 104,125 positive tweets, and 75,860 negative tweets. In the test split, there are 26,032 positive tweets, and 18,967 negative tweets. This is an uneven spread with more positive tweets, but nothing too extreme. There average length of tweets in both the training and testing data is about ~15 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4f97e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
    "temp_train_df = pd.read_csv(\"hf://datasets/gxb912/large-twitter-tweets-sentiment/train.csv\")\n",
    "temp_test_df = pd.read_csv(\"hf://datasets/gxb912/large-twitter-tweets-sentiment/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b4868787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (179995, 2)\n",
      "Testing: (44999, 2)\n",
      "\n",
      "Sentiment frequency\n",
      "Training data:\n",
      "1    104125\n",
      "0     75870\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "Testing data:\n",
      "1    26032\n",
      "0    18967\n",
      "Name: sentiment, dtype: int64\n",
      "Training:\n",
      "Average length of tweets: 15.184499569432484\n",
      "Testing:\n",
      "Average length of tweets: 15.159981332918509\n"
     ]
    }
   ],
   "source": [
    "# 0 = negative, 1 = positive\n",
    "# load twitter dataset into pandas \n",
    "from nltk.tokenize import TweetTokenizer \n",
    "\n",
    "print(\"Training: \" + str(temp_train_df.shape))\n",
    "print(\"Testing: \" + str(temp_test_df.shape))\n",
    "print()\n",
    "\n",
    "# Get sentiment counts\n",
    "print(\"Sentiment frequency\")\n",
    "print(\"Training data:\")\n",
    "print(temp_train_df['sentiment'].value_counts())\n",
    "print() \n",
    "print(\"Testing data:\")\n",
    "print(temp_test_df['sentiment'].value_counts())\n",
    "\n",
    "# Method for removing the \"@airline\" token from a tweet\n",
    "def _remove_airline_tok(tokens):\n",
    "    return tokens[1:] if tokens[0].startswith('@') else tokens   \n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "# Tokenize tweet text (training)\n",
    "tweet_text = temp_train_df[\"text\"].values\n",
    "tweet_tokenized = [tokenizer.tokenize(tweet) for tweet in tweet_text]\n",
    "train_clean_tweets = [_remove_airline_tok(tokens) for tokens in tweet_tokenized]\n",
    "\n",
    "# Tokenize tweet text (testing)\n",
    "tweet_text = temp_test_df[\"text\"].values\n",
    "tweet_tokenized = [tokenizer.tokenize(tweet) for tweet in tweet_text]\n",
    "test_clean_tweets = [_remove_airline_tok(tokens) for tokens in tweet_tokenized]\n",
    "    \n",
    "# Find the average length of tweets \n",
    "def average_tweet_length_finder(df):\n",
    "    len_arr = []\n",
    "    for tweet in df:\n",
    "        tweet_length = len(tweet)\n",
    "        len_arr.append(tweet_length)\n",
    "    tweet_mean = np.mean(len_arr)\n",
    "    print(\"Average length of tweets:\", tweet_mean)\n",
    "    \n",
    "print(\"Training:\")\n",
    "average_tweet_length_finder(train_clean_tweets)\n",
    "\n",
    "print(\"Testing:\")\n",
    "average_tweet_length_finder(test_clean_tweets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1bb37164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.64      0.68     18967\n",
      "           1       0.76      0.82      0.79     26032\n",
      "\n",
      "    accuracy                           0.75     44999\n",
      "   macro avg       0.74      0.73      0.74     44999\n",
      "weighted avg       0.74      0.75      0.74     44999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression on large-twitter-tweets dataset using word2vec vectors\n",
    "\n",
    "def tweet_to_avg_vector(tweet_tokens, w2v_vectors):\n",
    "    vectors = []\n",
    "    for word in tweet_tokens:\n",
    "        if word in w2v_vectors:  # Check if word is in the Word2Vec model\n",
    "            vectors.append(w2v_vectors[word])  # Get the word vector\n",
    "        else:\n",
    "            vectors.append(np.zeros(300))  # Use a zero vector for words not in the model\n",
    "    if vectors:  \n",
    "        return np.mean(vectors, axis=0)  # Average the word vectors to represent the whole tweet\n",
    "    else:\n",
    "        return np.zeros(300)  # Use a zero vector for words not in the model\n",
    "\n",
    "\n",
    "# Generate word2vec representations\n",
    "x_train_vectors = np.array([tweet_to_avg_vector(tweet, w2v_vectors) for tweet in train_clean_tweets])\n",
    "x_test_vectors = np.array([tweet_to_avg_vector(tweet, w2v_vectors) for tweet in test_clean_tweets])\n",
    "\n",
    "# Labels for training and testing data\n",
    "label_train = temp_train_df[\"sentiment\"]\n",
    "label_test = temp_test_df[\"sentiment\"]\n",
    "\n",
    "# Train a LOGISTIC regression model (using word2vec vectors)\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(x_train_vectors, label_train)\n",
    "\n",
    "# Get predictions then evaluate performance on test set\n",
    "label_pred = lr.predict(x_test_vectors)\n",
    "print(classification_report(label_test, label_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "70d2f786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.71      0.74     18967\n",
      "           1       0.80      0.85      0.82     26032\n",
      "\n",
      "    accuracy                           0.79     44999\n",
      "   macro avg       0.79      0.78      0.78     44999\n",
      "weighted avg       0.79      0.79      0.79     44999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression on large-twitter-tweets dataset using TF-idf vectors\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Convert tokenized tweets into strings that tf-idf vectorizer can actually use\n",
    "train_tweets_clean_joined = [' '.join(tokens) for tokens in train_clean_tweets]\n",
    "test_tweets_clean_joined = [' '.join(tokens) for tokens in test_clean_tweets]\n",
    "\n",
    "train_tweets_tfidf = vectorizer.fit_transform(train_tweets_clean_joined)\n",
    "test_tweets_tfidf = vectorizer.transform(test_tweets_clean_joined)\n",
    "\n",
    "\n",
    "# Train a multinomial logistic regression model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(train_tweets_tfidf, label_train)\n",
    "\n",
    "# Get predictions then evaluate performance on test set\n",
    "label_pred = lr.predict(test_tweets_tfidf)\n",
    "print(classification_report(label_test, label_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d20f48",
   "metadata": {},
   "source": [
    "### TF-idf or word2vec (large-twitter-tweets-sentiment)?\n",
    "\n",
    "#### Word2vec\n",
    "There are more positive tweets (104,125 in training, 26,032 in testing) than negative tweets (75,860 in training, 18,967 in testing) in the dataset, and that's reflected in the logistic regression classifier's performance. Negative tweets had an F1 score of 0.68, while positive tweets had an F1 score of 0.79. The macro and micro averages are the same though, both sitting at 0.74.\n",
    "\n",
    "#### TF-idf\n",
    "Again, the F1 score for the negative class (0.74) is lower than the F1 score for the positive class (0.82). In this case, the macro and micro averages were very similar with a macro average of 0.78 and a micro average of 0.79.\n",
    "\n",
    "\n",
    "Comparing the performance of the two models (logstic regression trained with word2vec vs tf-idf vectors), the TF-idf-trained model performed slightly better in this case, and it's faster, so I will be sticking with the tf-idf logistic regression model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4bf6b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "\n",
    "# Create function to convert numerical sentiment labels into text\n",
    "def map_sentiment(value):\n",
    "    if value == 4:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "twitter_df[\"sentiment_num_conv\"] = twitter_df[\"sentiment\"].apply(map_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1b07f6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.77    800000\n",
      "           1       0.75      0.84      0.79    800000\n",
      "\n",
      "    accuracy                           0.78   1600000\n",
      "   macro avg       0.79      0.78      0.78   1600000\n",
      "weighted avg       0.79      0.78      0.78   1600000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a random sample of 1000 rows to test code without taking forever\n",
    "# corpus_sample = twitter_df.sample(n=1000) \n",
    "\n",
    "\n",
    "####\n",
    "# Tokenize tweet text\n",
    "tweet_text = twitter_df[\"text\"].values  \n",
    "# tweet_text = corpus_sample[\"text\"].values\n",
    "tweet_tokenized = [tokenizer.tokenize(tweet) for tweet in tweet_text]\n",
    "corpus_tweets_clean = [_remove_airline_tok(tokens) for tokens in tweet_tokenized]\n",
    "\n",
    "# Convert tokenized tweets into strings that tf-idf vectorizer can actually use\n",
    "corpus_tweets_clean_joined = [' '.join(tokens) for tokens in corpus_tweets_clean]\n",
    "####\n",
    "\n",
    "# train_tweets_tfidf = vectorizer.fit_transform(train_tweets_clean_joined)\n",
    "corpus_tweets_tfidf = vectorizer.transform(corpus_tweets_clean_joined)\n",
    "\n",
    "\n",
    "# Get predictions then evaluate performance on test set\n",
    "corpus_label_pred = lr.predict(corpus_tweets_tfidf)\n",
    "print(classification_report(twitter_df[\"sentiment_num_conv\"], corpus_label_pred))\n",
    "# print(classification_report(corpus_sample[\"sentiment_num_conv\"], corpus_label_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9bede",
   "metadata": {},
   "source": [
    "#### Research Questions\n",
    "\n",
    "- How has sentiment related to popular keywords changed over time? \n",
    "\n",
    "- What’s the relationship between sentiment and popularity?\n",
    "\n",
    "- Potential sub-question: Which entities have the most positive or negative sentiment, on average? (bert)\n",
    "    \n",
    "- Maybe looking at some relational aspect between the tweets and what is being discussed/ topic (ner)\n",
    "\n",
    "- What are the most common entities in tweets associated with different sentiment labels?\n",
    "\n",
    "- What types of entities are the most common? Which entity types are more associated with each sentiment label? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d088d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random sample of 1000 rows to test code without taking forever\n",
    "# corpus_sample = twitter_df.sample(n=1000) \n",
    "\n",
    "\n",
    "####\n",
    "# Tokenize tweet text\n",
    "tweet_text = twitter_df[\"text\"].values  \n",
    "# tweet_text = corpus_sample[\"text\"].values\n",
    "tweet_tokenized = [tokenizer.tokenize(tweet) for tweet in tweet_text]\n",
    "corpus_tweets_clean = [_remove_airline_tok(tokens) for tokens in tweet_tokenized]\n",
    "\n",
    "# Convert tokenized tweets into strings that tf-idf vectorizer can actually use\n",
    "corpus_tweets_clean_joined = [' '.join(tokens) for tokens in corpus_tweets_clean]\n",
    "####\n",
    "\n",
    "# train_tweets_tfidf = vectorizer.fit_transform(train_tweets_clean_joined)\n",
    "corpus_tweets_tfidf = vectorizer.transform(corpus_tweets_clean_joined)\n",
    "\n",
    "\n",
    "# Get predictions then evaluate performance on test set\n",
    "corpus_label_pred = lr.predict(corpus_tweets_tfidf)\n",
    "print(classification_report(twitter_df[\"sentiment_num_conv\"], corpus_label_pred))\n",
    "# print(classification_report(corpus_sample[\"sentiment_num_conv\"], corpus_label_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c7e1c",
   "metadata": {},
   "source": [
    "### What’s the relationship between sentiment and popularity?\n",
    "Based on the top 10 most common spans, there doesn't really seem to be a relationship! If a label of 0 = negative sentiment and a 1 = positive sentiment, then on average, entities that appear in more positive tweets would have a sentiment closer to 1. Lookin at the top 10 most common spans and their average sentiments, we get this:\n",
    "\n",
    "Top 10 most common entity spans and their average sentiment (logistic regression prediction):\n",
    "  - today: 57597 instances || Average sentiment = 0.46\n",
    "  - tomorrow: 27349 instances || Average sentiment = 0.45\n",
    "  - 2: 26003 instances || Average sentiment = 0.49\n",
    "  - tonight: 23124 instances || Average sentiment = 0.52\n",
    "  - one: 14757 instances || Average sentiment = 0.6\n",
    "  - first: 12105 instances || Average sentiment = 0.69\n",
    "  - 4: 9864 instances || Average sentiment = 0.55\n",
    "  - last night: 9733 instances || Average sentiment = 0.43\n",
    "  - morning: 8907 instances || Average sentiment = 0.78\n",
    "  - yesterday: 8535 instances || Average sentiment = 0.43\n",
    "\n",
    "Top 10 most common entity spans and their average sentiment (twitter_df ground truth):\n",
    "  - today: 57597 instances || Average sentiment = 0.45\n",
    "  - tomorrow: 27349 instances || Average sentiment = 0.44\n",
    "  - 2: 26003 instances || Average sentiment = 0.46\n",
    "  - tonight: 23124 instances || Average sentiment = 0.5\n",
    "  - one: 14757 instances || Average sentiment = 0.56\n",
    "  - first: 12105 instances || Average sentiment = 0.64\n",
    "  - 4: 9864 instances || Average sentiment = 0.52\n",
    "  - last night: 9733 instances || Average sentiment = 0.41\n",
    "  - morning: 8907 instances || Average sentiment = 0.72\n",
    "  - yesterday: 8535 instances || Average sentiment = 0.42\n",
    "\n",
    "It looks like, aside from \"one,\" \"first,\" and \"morning,\" the ten most common entity spans generally have average sentiment scores around 0.5 or slightly below it! It seems that an entity being more common or popular doesn't make it related to more positive discussions. Another note is that the average sentiments generated from the sentiment predictions from the logistic regression model tend to be a bit higher than the averages generated from ground truth sentiment labels for the top 10 most common entities (ex: \"morning\" has an average sentiment of 0.78 according to the logistic regression labels, but a 0.72 based on ground truth sentiment labels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c7cdf927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common entity spans:\n",
      "  - today: 57597 instances\n",
      "  - tomorrow: 27349 instances\n",
      "  - 2: 26003 instances\n",
      "  - tonight: 23124 instances\n",
      "  - one: 14757 instances\n",
      "  - first: 12105 instances\n",
      "  - 4: 9864 instances\n",
      "  - last night: 9733 instances\n",
      "  - morning: 8907 instances\n",
      "  - yesterday: 8535 instances\n"
     ]
    }
   ],
   "source": [
    "# Initialize entity span counter\n",
    "entity_spans = Counter()\n",
    "\n",
    "# Iterate over each tweet and count spans\n",
    "for doc in doc_web:\n",
    "    for ent in doc.ents:\n",
    "        entity_spans.update([ent.text])  \n",
    "\n",
    "# Get top 10 most common spans\n",
    "most_common_ents = entity_spans.most_common(10)\n",
    "\n",
    "# Print the top 10 most common spans with their counts\n",
    "print(\"Top 10 most common entity spans:\")\n",
    "for span, count in most_common_ents:\n",
    "    print(f\"  - {span}: {count} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "89b72e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common entity spans and their average sentiment (logistic regression prediction):\n",
      "  - today: 57597 instances || Average sentiment = 0.46\n",
      "  - tomorrow: 27349 instances || Average sentiment = 0.45\n",
      "  - 2: 26003 instances || Average sentiment = 0.49\n",
      "  - tonight: 23124 instances || Average sentiment = 0.52\n",
      "  - one: 14757 instances || Average sentiment = 0.6\n",
      "  - first: 12105 instances || Average sentiment = 0.69\n",
      "  - 4: 9864 instances || Average sentiment = 0.55\n",
      "  - last night: 9733 instances || Average sentiment = 0.43\n",
      "  - morning: 8907 instances || Average sentiment = 0.78\n",
      "  - yesterday: 8535 instances || Average sentiment = 0.43\n",
      "\n",
      "Top 10 most common entity spans and their average sentiment (twitter_df ground truth):\n",
      "  - today: 57597 instances || Average sentiment = 0.45\n",
      "  - tomorrow: 27349 instances || Average sentiment = 0.44\n",
      "  - 2: 26003 instances || Average sentiment = 0.46\n",
      "  - tonight: 23124 instances || Average sentiment = 0.5\n",
      "  - one: 14757 instances || Average sentiment = 0.56\n",
      "  - first: 12105 instances || Average sentiment = 0.64\n",
      "  - 4: 9864 instances || Average sentiment = 0.52\n",
      "  - last night: 9733 instances || Average sentiment = 0.41\n",
      "  - morning: 8907 instances || Average sentiment = 0.72\n",
      "  - yesterday: 8535 instances || Average sentiment = 0.42\n"
     ]
    }
   ],
   "source": [
    "# Get the average sentiment associated with each of the top 10 most common spans\n",
    "\n",
    "# Initialize entity span counter and dictionary to track sentiment for each entity\n",
    "# entity_spans = Counter() COMM\n",
    "entity_sentiments = {}\n",
    "\n",
    "\n",
    "# Method for iterating through doc web and finding average of its sentiment across all tweets\n",
    "# Parameter \"label_source\" defines if you want the to use sentiment labels predicted by model, or the ground truth\n",
    "# from twitter_df\n",
    "def get_avg_sents(label_source):\n",
    "    # Connect each tweet in doc_web with its predicted sentiment\n",
    "    for i, (doc, sentiment) in enumerate(zip(doc_web, label_source)): \n",
    "        for ent in doc.ents:\n",
    "            # Update entity span count\n",
    "            # entity_spans.update([ent.text]) COMM\n",
    "\n",
    "            # Store sentiment for the entity in the dictionary\n",
    "            if ent.text not in entity_sentiments:\n",
    "                entity_sentiments[ent.text] = []  # Initialize the list for this entity\n",
    "            entity_sentiments[ent.text].append(sentiment)  # Append the sentiment\n",
    "\n",
    "    # most_common_ents = entity_spans.most_common(10) COMM\n",
    "\n",
    "    # Print the top 10 most common entity spans and their average sentiment\n",
    "    for span, count in most_common_ents:\n",
    "        if span in entity_sentiments:\n",
    "            # Calculate the average sentiment for the current entity span\n",
    "            avg_sentiment = np.mean(entity_sentiments[span])  # Average of sentiment values\n",
    "\n",
    "            # Silly but helpful resource: https://www.geeksforgeeks.org/round-function-python/\n",
    "            print(f\"  - {span}: {count} instances || Average sentiment = {round(avg_sentiment, 2)}\")\n",
    "\n",
    "print(\"Top 10 most common entity spans and their average sentiment (logistic regression prediction):\")\n",
    "get_avg_sents(corpus_label_pred)\n",
    "print()\n",
    "print(\"Top 10 most common entity spans and their average sentiment (twitter_df ground truth):\")\n",
    "get_avg_sents(twitter_df[\"sentiment_num_conv\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589854a0",
   "metadata": {},
   "source": [
    "#### Which entities have the most positive or negative sentiment, on average? (bert)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
