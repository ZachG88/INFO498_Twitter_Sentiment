{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a50b31bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in training dataset (Airline sentiment tweet data)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "airline_df = pd.read_csv(\"hf://datasets/osanseviero/twitter-airline-sentiment/Tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45ba8d",
   "metadata": {},
   "source": [
    "### Dataset Description: Twitter Airline Sentiment\n",
    "The Twitter Airline Sentiment dataset originally came from Crowdflower's \"Data for Everyone\" library. It was originally created for a sentiment analysis, specifically in an attempt to find problems that customers noticed with each airline in the dataset.  Contributors were asked to classify the tweets as positive, neutral, or negative, and also asked to explain what issue was present in a tweet if it was negative (ex: \"delayed flight\"). Each tweet is also recorded with data like when it was created, the user who created the tweet, the sentiment label given to the tweet, and the airline the tweet is associated with.\n",
    "\n",
    "There are 14,640 tweets total in the dataset, and the tweets were scraped from the time period of February 16th, 2015 to February 24th, 2015. The average tweet length in the dataset is 19.12 words. \n",
    "\n",
    "We will be using this dataset to train our sentiment classification models.\n",
    "\n",
    "Tweet distribution per airline:\n",
    "United: 3822 || US Airways: 2913 || American: 2759 || Southwest: 2420 || Delta: 2222 || Virgin America: 504 \n",
    "\n",
    "Tweet distrubtion per sentiment:\n",
    "Negative: 9178 || Neutral: 3099 || Positive: 2363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2593182d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 15)\n",
      "2015-02-16 23:36:05 -0800\n",
      "2015-02-24 11:53:37 -0800\n",
      "{'Virgin America', 'Delta', 'US Airways', 'United', 'American', 'Southwest'}\n",
      "\n",
      "Tweet counts for airlines\n",
      "United            3822\n",
      "US Airways        2913\n",
      "American          2759\n",
      "Southwest         2420\n",
      "Delta             2222\n",
      "Virgin America     504\n",
      "Name: airline, dtype: int64\n",
      "\n",
      "Sentiment frequency\n",
      "negative    9178\n",
      "neutral     3099\n",
      "positive    2363\n",
      "Name: airline_sentiment, dtype: int64\n",
      "\n",
      "Average length of tweets: 19.122677595628414\n"
     ]
    }
   ],
   "source": [
    "# load twitter dataset into pandas \n",
    "from nltk.tokenize import TweetTokenizer \n",
    "\n",
    "print(airline_df.shape)\n",
    "print(min(airline_df['tweet_created'])) # Find earliest tweet for dataset\n",
    "print(max(airline_df['tweet_created'])) # Find earliest tweet for dataset\n",
    "\n",
    "print(set(airline_df['airline'])) # Find the unique airlines\n",
    "\n",
    "# Get airline tweet counts\n",
    "print()\n",
    "print(\"Tweet counts for airlines\")\n",
    "print(airline_df['airline'].value_counts())\n",
    "print()\n",
    "\n",
    "\n",
    "# Get airline sentiment counts\n",
    "print(\"Sentiment frequency\")\n",
    "print(airline_df['airline_sentiment'].value_counts())\n",
    "print() \n",
    "\n",
    "# Method for removing the \"@airline\" token from a tweet\n",
    "def _remove_airline_tok(tokens):\n",
    "    return tokens[1:] if tokens[0].startswith('@') else tokens   \n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "# Tokenize tweet text\n",
    "tweet_text = airline_df[\"text\"].values\n",
    "tweet_tokenized = [tokenizer.tokenize(tweet) for tweet in tweet_text]\n",
    "clean_tweets = [_remove_airline_tok(tokens) for tokens in tweet_tokenized]\n",
    "    \n",
    "# Find the average length of tweets\n",
    "len_arr = []\n",
    "for tweet in clean_tweets:\n",
    "    tweet_length = len(tweet)\n",
    "    len_arr.append(tweet_length)\n",
    "tweet_mean = np.mean(len_arr)\n",
    "print(\"Average length of tweets:\", tweet_mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04379b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(airline_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cc94e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful resource for multinomial regression: \n",
    "# https://machinelearningmastery.com/multinomial-logistic-regression-with-python/\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the airline data into training and test splits\n",
    "\n",
    "air_feats = airline_df[\"text\"]\n",
    "air_labels = airline_df[\"airline_sentiment\"]\n",
    "\n",
    "feat_train, feat_test, label_train, label_test = train_test_split(air_feats, air_labels, test_size = 0.8)\n",
    "\n",
    "# Tokenize the airline tweets in either split\n",
    "\n",
    "def _remove_airline_tok(tokens):\n",
    "    return tokens[1:] if tokens[0].startswith('@') else tokens    \n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "#Tokenizing training airline tweets\n",
    "train_tweets = feat_train.values\n",
    "train_tweets_tokenized = [tokenizer.tokenize(tweet) for tweet in train_tweets]\n",
    "train_tweets_clean = [_remove_airline_tok(tokens) for tokens in train_tweets_tokenized]\n",
    "\n",
    "#Tokenizing testing airline tweets\n",
    "test_tweets = feat_test.values\n",
    "test_tweets_tokenized = [tokenizer.tokenize(tweet) for tweet in test_tweets]\n",
    "test_tweets_clean = [_remove_airline_tok(tokens) for tokens in test_tweets_tokenized]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe2c9f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading.\n"
     ]
    }
   ],
   "source": [
    "# loading word2vec vectors\n",
    "\n",
    "# Useful resource on word2vec \n",
    "# https://medium.com/swlh/sentiment-classification-using-word-embeddings-word2vec-aedf28fbb8ca\n",
    "\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v_file = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "w2v_file = os.path.expanduser(w2v_file)\n",
    "\n",
    "\n",
    "w2v_vectors = KeyedVectors.load_word2vec_format(w2v_file, binary=True)\n",
    "print('done loading.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71fa135a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.96      0.86      7354\n",
      "     neutral       0.67      0.35      0.46      2476\n",
      "    positive       0.80      0.54      0.64      1882\n",
      "\n",
      "    accuracy                           0.76     11712\n",
      "   macro avg       0.75      0.62      0.65     11712\n",
      "weighted avg       0.75      0.76      0.74     11712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training multinomial regression using word2vec vectors\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Helpful guide: https://medium.com/swlh/sentiment-classification-using-word-embeddings-word2vec-aedf28fbb8ca\n",
    "\n",
    "\n",
    "\n",
    "# Function for creating the average word2vec vector for each tweet \n",
    "def tweet_to_avg_vector(tweet_tokens):\n",
    "    vectors = []\n",
    "    for word in tweet_tokens:\n",
    "        if word in w2v_vectors:\n",
    "            vectors.append(w2v_vectors[word])\n",
    "        else:\n",
    "            vectors.append(np.zeros(300))  # If word is not in word2Vec, use zero vector\n",
    "    return np.mean(vectors, axis=0)  # Average word vectors to represent the whole sentence \n",
    "\n",
    "# Convert training tweets to their average Word2Vec vector representations\n",
    "x_train_vectors = np.array([tweet_to_avg_vector(tweet) for tweet in train_tweets_clean])\n",
    "\n",
    "\n",
    "# Convert testing tweets to their average Word2Vec vector representations\n",
    "x_test_vectors = np.array([tweet_to_avg_vector(tweet) for tweet in test_tweets_clean])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train a multinomial logistic regression model (using word2vec vectors)\n",
    "mr = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "mr.fit(x_train_vectors, label_train)\n",
    "\n",
    "# Get predictions then evaluate performance on test set\n",
    "label_pred = mr.predict(x_test_vectors)\n",
    "print(classification_report(label_test, label_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "146db967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.97      0.85      7354\n",
      "     neutral       0.69      0.36      0.48      2476\n",
      "    positive       0.84      0.45      0.59      1882\n",
      "\n",
      "    accuracy                           0.76     11712\n",
      "   macro avg       0.76      0.59      0.64     11712\n",
      "weighted avg       0.76      0.76      0.73     11712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training multinomial regression using TF-IDF vectors \n",
    "\n",
    "# Get TF-IDF vectors after tokenizing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Convert tokenized tweets into strings that tf-idf vectorizer can actually use\n",
    "train_tweets_clean_joined = [' '.join(tokens) for tokens in train_tweets_clean]\n",
    "test_tweets_clean_joined = [' '.join(tokens) for tokens in test_tweets_clean]\n",
    "\n",
    "train_tweets_tfidf = vectorizer.fit_transform(train_tweets_clean_joined)\n",
    "test_tweets_tfidf = vectorizer.transform(test_tweets_clean_joined)\n",
    "\n",
    "\n",
    "# Train a multinomial logistic regression model\n",
    "mr = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "mr.fit(train_tweets_tfidf, label_train)\n",
    "\n",
    "# Get predictions then evaluate performance on test set\n",
    "label_pred = mr.predict(test_tweets_tfidf)\n",
    "print(classification_report(label_test, label_pred))\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c46d878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n              \\n\\nword2vec:\\n              precision    recall  f1-score   support\\n\\n    negative       0.77      0.96      0.85      7333\\n     neutral       0.67      0.36      0.47      2471\\n    positive       0.82      0.50      0.62      1908\\n\\n    accuracy                           0.76     11712\\n   macro avg       0.75      0.61      0.65     11712\\nweighted avg       0.75      0.76      0.74     11712\\n\\n\\ntf-idf:\\n              precision    recall  f1-score   support\\n\\n    negative       0.75      0.97      0.84      7333\\n     neutral       0.68      0.35      0.46      2471\\n    positive       0.84      0.42      0.56      1908\\n\\n    accuracy                           0.75     11712\\n   macro avg       0.76      0.58      0.62     11712\\nweighted avg       0.75      0.75      0.72     11712\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "              \n",
    "\n",
    "word2vec:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "    negative       0.77      0.96      0.85      7333\n",
    "     neutral       0.67      0.36      0.47      2471\n",
    "    positive       0.82      0.50      0.62      1908\n",
    "\n",
    "    accuracy                           0.76     11712\n",
    "   macro avg       0.75      0.61      0.65     11712\n",
    "weighted avg       0.75      0.76      0.74     11712\n",
    "\n",
    "\n",
    "tf-idf:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "    negative       0.75      0.97      0.84      7333\n",
    "     neutral       0.68      0.35      0.46      2471\n",
    "    positive       0.84      0.42      0.56      1908\n",
    "\n",
    "    accuracy                           0.75     11712\n",
    "   macro avg       0.76      0.58      0.62     11712\n",
    "weighted avg       0.75      0.75      0.72     11712\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867aa7e7",
   "metadata": {},
   "source": [
    "### TF-idf or word2vec (airline data)?\n",
    "\n",
    "#### Word2vec\n",
    "Per class performance of the multinomial regression model trained on word2vec:\n",
    "Negative: Precision = 0.77 || Recall = 0.96 || F1-score = 0.85\n",
    "Neutral: Precision = 0.67 || Recall = 0.36 || F1-score = 0.47\n",
    "Positive: Precision = 0.82 || Recall = 0.50 || F1-score = 0.62\n",
    "\n",
    "The airline dataset is imbalanced with a lot of negative tweets, and the model seems to have gotten better at classifying negative tweets than neutral or positive ones. Looking at the recall scores specifically, we can see that the model correctly classied 96% of the negative tweets, but much lower for the neutral (36%) and positive (50%) tweets. This is reflected in the per-class F1 scores as well, with the negative class F1 score being at a solid 0.85, while the neutral (0.47) and positive (0.62) F1 scores are much lower.\n",
    "\n",
    "The average accuracy of the multinomial regression model trained on word2vec vectors is 0.76. The macro-averaged F1-score is 0.65, while the micro-averaged F1-score is 0.74. This makes sense, as the higher number of negative tweets and the model's superior performance on those negative tweets would bring the micro-F1 up, while the macro-F1 reflects that it did worse on classifying neutral and positive tweets.\n",
    "\n",
    "\n",
    "#### TF-idf\n",
    "\n",
    "Per class performance of the multinomial regression model trained on TF-idf vectors:\n",
    "Negative: Precision = 0.75 || Recall = 0.97 || F1-score = 0.84\n",
    "Neutral: Precision = 0.68 || Recall = 0.35 || F1-score = 0.46\n",
    "Positive: Precision = 0.84 || Recall = 0.42 || F1-score = 0.57\n",
    "\n",
    "Again, the imbalance in the distribution of sentiments in the tweets is visible here. The model performed better on negative tweets (F1-score: 0.84) than it did on neutral (F1-score: 0.46) or positive tweets (F1-score: 0.57). Additionally, we can see the same pattern where the micro-F1 score, which gives equal weight to all instances, is biased by the high number of negative tweets and sits at 0.72 while the macro-average, which gives equal weight to all classes, is lower at 0.62.\n",
    "\n",
    "Comparing the averages between the two models, it seems that word2vec performs slightly better but it's a small difference and word2vec takes much longer to run, so I'd just stick to using the TF-idf vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f8ddcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the main Twitter corpus\n",
    "twitter_df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding =\"latin-1\", \n",
    "                         names=[\"sentiment\", \"id\", \"date\", \"flag\", \"user\", \"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ff53d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id                          date      flag  \\\n",
       "0                0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1                0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2                0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3                0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4                0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...            ...         ...                           ...       ...   \n",
       "1599995          4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996          4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997          4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998          4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999          4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(twitter_df.shape)\n",
    "display(twitter_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b626b",
   "metadata": {},
   "source": [
    "### Dataset Description: Twitter Sentiment140\n",
    "The Sentiment140 dataset is a dataset of Tweets that was collected as part of a research paper which can be found at this link: (https://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf).\n",
    "\n",
    "The original use of the dataset was test how machine learning models would perform when classifying the sentiment of Tweets when trained on data including emoticons like \":)\". Rather than hand-labeling data, they used the emoticons as a noisy label for the sentiment of each tweet. Tweets with emoticons like \":)\" were labeled as containing positive sentiment, while tweets with emoticons like \":(\" were labeled as negative. \n",
    "\n",
    "The Tweets were gathered through various queries using Twitter's API, and tweets containing both positive and negative emoticons were removed from the dataset. Each tweet is also recorded with data like when it was created, the user who created the tweet, the sentiment label given to the tweet, and the query used to find the tweet, if any.\n",
    "\n",
    "There are 1,600,000 Tweets in the dataset, and there are tweets from 659,775 different users. The tweets are taken from the time period of April 17th, 2009 to May 27th, 2009. The distribution of sentiments is even, with 800,000 positive tweets and 800,000 negative tweets. The average length of tweets in the corpus is 14.85 words.\n",
    "\n",
    "We will be using this dataset as the corpus we're analyzing.\n",
    "\n",
    "##### Note: Realized Kaggle tricked me because the Sentiment140 dataset page had a note that said \"target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\" but there are actually only positive / negative labels in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12de8f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "\n",
    "# Create function to convert numerical sentiment labels into text\n",
    "def map_sentiment(value):\n",
    "    if value == 0:\n",
    "        return 'negative'\n",
    "    elif value == 2:\n",
    "        return 'neutral'\n",
    "    elif value == 4:\n",
    "        return 'positive'\n",
    "\n",
    "twitter_df[\"sentiment_text\"] = twitter_df[\"sentiment\"].apply(map_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bf1ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(twitter_df[\"sentiment\"].head(-40))\n",
    "# print(twitter_df[\"sentiment_text\"].head(-40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8221973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr 17 20:30:31 PDT 2009\n",
      "Wed May 27 07:27:38 PDT 2009\n",
      "Unique user count: 659775\n",
      "Sentiment frequency\n",
      "0    800000\n",
      "4    800000\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "Average length of tweets: 14.852441875\n"
     ]
    }
   ],
   "source": [
    "print(min(twitter_df['date'])) # Find earliest tweet for dataset\n",
    "print(max(twitter_df['date'])) # Find earliest tweet for dataset\n",
    "\n",
    "# Find number of different users in the dataset\n",
    "unique_users = set(twitter_df[\"user\"])\n",
    "unique_user_count = len(unique_users)\n",
    "print(\"Unique user count:\", unique_user_count)\n",
    "\n",
    "# Get frequency of each sentiment\n",
    "print(\"Sentiment frequency\")\n",
    "print(twitter_df['sentiment'].value_counts())\n",
    "print() \n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "# Tokenize tweet text\n",
    "# also removing the \"@person\" token from a tweet; cleans up any tweets that are replies to other users\n",
    "tweet_text = twitter_df[\"text\"].values\n",
    "tweet_tokenized = [tokenizer.tokenize(tweet) for tweet in tweet_text]\n",
    "clean_tweets = [_remove_airline_tok(tokens) for tokens in tweet_tokenized]\n",
    "    \n",
    "# Find the average length of tweets\n",
    "len_arr = []\n",
    "for tweet in clean_tweets:\n",
    "    tweet_length = len(tweet)\n",
    "    len_arr.append(tweet_length)\n",
    "tweet_mean = np.mean(len_arr)\n",
    "print(\"Average length of tweets:\", tweet_mean)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88b26a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER entity types:\n",
    "# PERSON:      People, including fictional.\n",
    "# NORP:        Nationalities or religious or political groups.\n",
    "# FAC:         Buildings, airports, highways, bridges, etc.\n",
    "# ORG:         Companies, agencies, institutions, etc.\n",
    "# GPE:         Countries, cities, states.\n",
    "# LOC:         Non-GPE locations, mountain ranges, bodies of water.\n",
    "# PRODUCT:     Objects, vehicles, foods, etc. (Not services.)\n",
    "# EVENT:       Named hurricanes, battles, wars, sports events, etc.\n",
    "# WORK_OF_ART: Titles of books, songs, etc.\n",
    "# LAW:         Named documents made into laws.\n",
    "# LANGUAGE:    Any named language.\n",
    "# DATE:        Absolute or relative dates or periods.\n",
    "# TIME:        Times smaller than a day.\n",
    "# PERCENT:     Percentage, including ”%“.\n",
    "# MONEY:       Monetary values, including unit.\n",
    "# QUANTITY:    Measurements, as of weight or distance.\n",
    "# ORDINAL:     “first”, “second”, etc.\n",
    "# CARDINAL:    Numerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc38a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time for NER!\n",
    "\n",
    "# Load spacy models and using for NER\n",
    "import spacy\n",
    "\n",
    "# load a spacy model trained on web text\n",
    "nlp_web = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc_web = twitter_df['text'].apply(nlp_web) # Apply nlp web to each row in the ['document'] series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16f86431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [(@switchfoot http://twitpic.com/2y1zl -, PERS...\n",
       "1                                            [(School, ORG)]\n",
       "2                                           [(50%, PERCENT)]\n",
       "3                                                         []\n",
       "4                                                         []\n",
       "                                 ...                        \n",
       "1599995                                                   []\n",
       "1599996                                     [(Walt, PERSON)]\n",
       "1599997                                                   []\n",
       "1599998      [(38th, ORDINAL), (Tupac Amaru Shakur, PERSON)]\n",
       "1599999                                      [(#, CARDINAL)]\n",
       "Name: text, Length: 1600000, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_entities(doc): # Function for getting the entity text + entity labels for each doc \n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "entities = doc_web.apply(extract_entities)\n",
    "display(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "059dd639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of entities per tweet: 0.934395625\n",
      "PERSON         344579.0\n",
      "DATE           291742.0\n",
      "ORG            305084.0\n",
      "PERCENT          2850.0\n",
      "ORDINAL         26742.0\n",
      "CARDINAL       158404.0\n",
      "LOC             11457.0\n",
      "TIME           114966.0\n",
      "GPE            138184.0\n",
      "NORP            36439.0\n",
      "QUANTITY         5938.0\n",
      "WORK_OF_ART     12484.0\n",
      "FAC              8745.0\n",
      "MONEY           13660.0\n",
      "PRODUCT         16398.0\n",
      "LANGUAGE         3032.0\n",
      "EVENT            2842.0\n",
      "LAW              1487.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "entity_sum = 0\n",
    "for doc in entities:\n",
    "    entity_count = len(doc)\n",
    "    entity_sum += entity_count\n",
    "    \n",
    "entity_avg = entity_sum / len(entities)\n",
    "print(\"Average number of entities per tweet:\", entity_avg)\n",
    "\n",
    "\n",
    "# Now find total number of each entity type\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Function to count entity types in a single document\n",
    "def count_entity_types(doc):\n",
    "    entity_types = [ent.label_ for ent in doc.ents]\n",
    "    return Counter(entity_types)\n",
    "\n",
    "entity_counts = doc_web.apply(count_entity_types)\n",
    "\n",
    "# Convert to dataframe to make summarizing easier\n",
    "entity_counts_df = pd.DataFrame(list(entity_counts))\n",
    "\n",
    "# Find the sum of instances for each entity type\n",
    "sum_entity_counts = entity_counts_df.sum()\n",
    "\n",
    "print(sum_entity_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ba57b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON: \n",
      "  - Twitter: 3712 instances\n",
      "  - Lol: 1917 instances\n",
      "  - @ddlovato: 1636 instances\n",
      "DATE: \n",
      "  - today: 57597 instances\n",
      "  - tomorrow: 27349 instances\n",
      "  - yesterday: 8535 instances\n",
      "ORG: \n",
      "  - LOL: 8055 instances\n",
      "  - iPod: 1262 instances\n",
      "  - LOVE: 1146 instances\n",
      "PERCENT: \n",
      "  - 100%: 587 instances\n",
      "  - 50%: 134 instances\n",
      "  - 10%: 124 instances\n",
      "ORDINAL: \n",
      "  - first: 12105 instances\n",
      "  - second: 2461 instances\n",
      "  - 1st: 1539 instances\n",
      "CARDINAL: \n",
      "  - 2: 25610 instances\n",
      "  - one: 14757 instances\n",
      "  - 4: 9613 instances\n",
      "LOC: \n",
      "  - NYC: 1093 instances\n",
      "  - Europe: 512 instances\n",
      "  - new moon: 299 instances\n",
      "TIME: \n",
      "  - tonight: 23124 instances\n",
      "  - last night: 9733 instances\n",
      "  - morning: 8907 instances\n",
      "GPE: \n",
      "  - London: 1901 instances\n",
      "  - LA: 1891 instances\n",
      "  - UK: 1883 instances\n",
      "NORP: \n",
      "  - french: 1109 instances\n",
      "  - english: 775 instances\n",
      "  - Congrats: 630 instances\n",
      "QUANTITY: \n",
      "  - a ton: 273 instances\n",
      "  - a mile: 73 instances\n",
      "  - 100 degrees: 57 instances\n",
      "WORK_OF_ART: \n",
      "  - Love: 2551 instances\n",
      "  - Star Trek: 355 instances\n",
      "  - LoL: 225 instances\n",
      "FAC: \n",
      "  - Disneyland: 205 instances\n",
      "  - metro: 158 instances\n",
      "  - True Blood: 152 instances\n",
      "MONEY: \n",
      "  - #fb: 481 instances\n",
      "  - 1: 395 instances\n",
      "  - 2: 329 instances\n",
      "PRODUCT: \n",
      "  - Twitter: 3008 instances\n",
      "  - â¥: 292 instances\n",
      "  - Magic: 135 instances\n",
      "LANGUAGE: \n",
      "  - English: 1181 instances\n",
      "  - english: 1021 instances\n",
      "  - French: 104 instances\n",
      "EVENT: \n",
      "  - Wimbledon: 125 instances\n",
      "  - the French Open: 77 instances\n",
      "  - @gfalcone601: 55 instances\n",
      "LAW: \n",
      "  - TONS: 35 instances\n",
      "  - Thnx 4: 14 instances\n",
      "  - ï¿½: 12 instances\n"
     ]
    }
   ],
   "source": [
    "entity_labels = ['PERSON', 'DATE', 'ORG', 'PERCENT', 'ORDINAL', 'CARDINAL', 'LOC', 'TIME', 'GPE', 'NORP', 'QUANTITY',\n",
    "                 'WORK_OF_ART', 'FAC', 'MONEY', 'PRODUCT', 'LANGUAGE', 'EVENT', 'LAW']\n",
    "\n",
    "# Dictionary to store counts for each type\n",
    "entity_spans = {label: Counter() for label in entity_labels}\n",
    "\n",
    "# Iterate over each document then count spans\n",
    "for doc in doc_web:\n",
    "    for ent in doc.ents:\n",
    "        entity_spans[ent.label_].update([ent.text]) \n",
    "\n",
    "\n",
    "# Helpful guide on using items() to iterate through keys+values in a dict: \n",
    "# https://www.geeksforgeeks.org/iterate-over-a-dictionary-in-python/\n",
    "# Finding most common item in a list:\n",
    "# https://stackoverflow.com/questions/3594514/how-to-find-most-common-elements-of-a-list\n",
    "most_common_ents = {}\n",
    "for entity, spans in entity_spans.items():\n",
    "    if spans:  # Confirm there are named entities in the doc\n",
    "        \n",
    "        #Only shows the top 1 most common span\n",
    "        #most_common_span, count = spans.most_common(1)[0]  # Get the most common span and its count\n",
    "        #most_common_ents[entity] = (most_common_span, count)  # Add it to the result dictionary\n",
    "        \n",
    "        # Show top 3 common spans\n",
    "        most_common_spans = spans.most_common(3)  # Get the top 3 most common spans and their counts\n",
    "        most_common_ents[entity] = most_common_spans  # Add them to the result dictionary\n",
    "    \n",
    "    \n",
    "# Print the most common span for each entity type\n",
    "# for entity, (span, count) in most_common_ents.items():\n",
    "#     print(entity + \": Most common span = \" + span + \" (\"+ str(count) + \" instances)\")\n",
    "    \n",
    "# Print the top 3 most common spans for each entity type\n",
    "for entity, common_spans in most_common_ents.items():\n",
    "    print(entity + \": \")\n",
    "    for span, count in common_spans:\n",
    "        print(f\"  - {span}: {count} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed77b0e",
   "metadata": {},
   "source": [
    "### Baseline entity analysis across all tweets \n",
    "There are 0.93 entities in each tweet, on average. The most common type of entity is \"PERSON\" (344,579 tweets), followed by \"ORG\" (305,084 tweets), and \"DATE\" (291,742) tweets. The least common entity types are \"LAW\" (1,487 tweets), \"EVENT\" (2,842 teets), and \"PERCENT\" (2,850 tweets).\n",
    "\n",
    "The most common spans per entity type are as follows:\n",
    "- PERSON: \"Twitter\", \"Lol\", and \"@ddlovato\"\n",
    "- DATE: \"Today\", \"tomorrow\", and \"yesterday\"\n",
    "- ORG: \"LOL\", \"iPod\", and \"LOVE\"\n",
    "- PERCENT: \"100%\", \"50%\", and \"10%\"\n",
    "- ORDINAL: \"first\", \"second\", and \"1st\"\n",
    "- CARDINAL: \"2\", \"one\", and \"4\"\n",
    "- LOC: \"NYC\", \"Europe\", and \"new moon\"\n",
    "- TIME: \"tonight\", \"last night\", and \"morning\"\n",
    "- GPE: \"London\", \"LA\", and \"UK\"\n",
    "- NORP: \"french\", \"english\", and \"Congrats\"\n",
    "- QUANTITY: \"a ton\", \"a mile\", and \"100 degrees\"\n",
    "- WORK_OF_ART: \"Love\", \"Star Trek\" and \"LoL\"\n",
    "- FAC: \"Disneyland\", \"metro\", and \"True Blood\"\n",
    "- MONEY: \"#fb\", \"1\", and \"2\"\n",
    "- PRODUCT: \"Twitter\", \"â¥\", and \"Magic\"\n",
    "- LANGUAGE: \"English\", \"english\", \"French\"\n",
    "- EVENT: \"Wimbledon\", \"the French Open\", \"@gfalcone601\"\n",
    "- LAW: \"TONS\", \"Thnx 4\", \"ï¿½\"\n",
    "\n",
    "We can see that there are some interesting parts of these results, like the NER algorithm treating \"English\" and \"english\" as different languages, or treating \"English\" and \"english\" as different languages, though we can't just remove all capitalization as I'm sure that plays a role in helping the NER model to decide what is or isn't an entity. We can definitely see that these entites were pulled from the past. Entitiesl like \"iPod\" and \"@ddlovato\" (Demi Lovato) are definitely less culturally relevant now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b1e605d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tduon\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\tduon\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.88      0.68    800000\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       0.82      0.14      0.24    800000\n",
      "\n",
      "    accuracy                           0.51   1600000\n",
      "   macro avg       0.46      0.34      0.31   1600000\n",
      "weighted avg       0.69      0.51      0.46   1600000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tduon\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Get a random sample of 1000 rows to test code without taking forever\n",
    "# corpus_sample = twitter_df.sample(n=1000) \n",
    "# For full dataset, use \"twitter_df\" instead to replace \"corpus_sample\"\n",
    "\n",
    "####\n",
    "# Tokenize tweet text\n",
    "tweet_text = twitter_df[\"text\"].values  \n",
    "# tweet_text = corpus_sample[\"text\"].values\n",
    "tweet_tokenized = [tokenizer.tokenize(tweet) for tweet in tweet_text]\n",
    "corpus_tweets_clean = [_remove_airline_tok(tokens) for tokens in tweet_tokenized]\n",
    "\n",
    "# Convert tokenized tweets into strings that tf-idf vectorizer can actually use\n",
    "corpus_tweets_clean_joined = [' '.join(tokens) for tokens in corpus_tweets_clean]\n",
    "####\n",
    "\n",
    "# train_tweets_tfidf = vectorizer.fit_transform(train_tweets_clean_joined)\n",
    "corpus_tweets_tfidf = vectorizer.transform(corpus_tweets_clean_joined)\n",
    "\n",
    "\n",
    "# Get predictions then evaluate performance on test set\n",
    "corpus_label_pred = mr.predict(corpus_tweets_tfidf)\n",
    "print(classification_report(twitter_df[\"sentiment_text\"], corpus_label_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3584dc",
   "metadata": {},
   "source": [
    "### Uh oh, we need new training data\n",
    "Realized here that the training data having a \"neutral\" label but the corpus itself not having \"neutral\" tweets in it would be a problem. The trained model performed horribly (0.52 average accuracy, 0.31 macro f1 score, 0.47 micro f1 score)! This would also mean it's a binary classification task, so I should switch to logistic regression. No more multinomial regression situation!\n",
    "\n",
    "### Dataset Description: large-twitter-tweets-sentiment\n",
    "Pulled from Huggingface (link: https://huggingface.co/datasets/gxb912/large-twitter-tweets-sentiment), there are 224,994 tweets in it total, with 179,995 tweets in the training split and 44,999 tweets in the testing split. The dataset didn't include any data aside from the text and sentiment label of each tweet. There was no other information on the source of the dataset aside from the fact that it was annotated specifically for sentiment analysis.\n",
    "\n",
    "In the training split, there are 104,125 positive tweets, and 75,860 negative tweets. In the test split, there are 26,032 positive tweets, and 18,967 negative tweets. This is an uneven spread with more positive tweets, but nothing too extreme. There average length of tweets in both the training and testing data is about ~15 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f97e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = {'train': 'train.csv', 'test': 'test.csv'}\n",
    "temp_train_df = pd.read_csv(\"hf://datasets/gxb912/large-twitter-tweets-sentiment/train.csv\")\n",
    "temp_test_df = pd.read_csv(\"hf://datasets/gxb912/large-twitter-tweets-sentiment/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4868787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (179995, 2)\n",
      "Testing: (44999, 2)\n",
      "\n",
      "Sentiment frequency\n",
      "Training data:\n",
      "1    104125\n",
      "0     75870\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "Testing data:\n",
      "1    26032\n",
      "0    18967\n",
      "Name: sentiment, dtype: int64\n",
      "Training:\n",
      "Average length of tweets: 15.184499569432484\n",
      "Testing:\n",
      "Average length of tweets: 15.159981332918509\n"
     ]
    }
   ],
   "source": [
    "# 0 = negative, 1 = positive\n",
    "# load twitter dataset into pandas \n",
    "from nltk.tokenize import TweetTokenizer \n",
    "\n",
    "print(\"Training: \" + str(temp_train_df.shape))\n",
    "print(\"Testing: \" + str(temp_test_df.shape))\n",
    "print()\n",
    "\n",
    "# Get sentiment counts\n",
    "print(\"Sentiment frequency\")\n",
    "print(\"Training data:\")\n",
    "print(temp_train_df['sentiment'].value_counts())\n",
    "print() \n",
    "print(\"Testing data:\")\n",
    "print(temp_test_df['sentiment'].value_counts())\n",
    "\n",
    "# Method for removing the \"@airline\" token from a tweet\n",
    "def _remove_airline_tok(tokens):\n",
    "    return tokens[1:] if tokens[0].startswith('@') else tokens   \n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "# Tokenize tweet text (training)\n",
    "tweet_text = temp_train_df[\"text\"].values\n",
    "tweet_tokenized = [tokenizer.tokenize(tweet) for tweet in tweet_text]\n",
    "train_clean_tweets = [_remove_airline_tok(tokens) for tokens in tweet_tokenized]\n",
    "\n",
    "# Tokenize tweet text (testing)\n",
    "tweet_text = temp_test_df[\"text\"].values\n",
    "tweet_tokenized = [tokenizer.tokenize(tweet) for tweet in tweet_text]\n",
    "test_clean_tweets = [_remove_airline_tok(tokens) for tokens in tweet_tokenized]\n",
    "    \n",
    "# Find the average length of tweets \n",
    "def average_tweet_length_finder(df):\n",
    "    len_arr = []\n",
    "    for tweet in df:\n",
    "        tweet_length = len(tweet)\n",
    "        len_arr.append(tweet_length)\n",
    "    tweet_mean = np.mean(len_arr)\n",
    "    print(\"Average length of tweets:\", tweet_mean)\n",
    "    \n",
    "print(\"Training:\")\n",
    "average_tweet_length_finder(train_clean_tweets)\n",
    "\n",
    "print(\"Testing:\")\n",
    "average_tweet_length_finder(test_clean_tweets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bb37164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.64      0.68     18967\n",
      "           1       0.76      0.82      0.79     26032\n",
      "\n",
      "    accuracy                           0.75     44999\n",
      "   macro avg       0.74      0.73      0.74     44999\n",
      "weighted avg       0.74      0.75      0.74     44999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression on large-twitter-tweets dataset using word2vec vectors\n",
    "\n",
    "def tweet_to_avg_vector(tweet_tokens, w2v_vectors):\n",
    "    vectors = []\n",
    "    for word in tweet_tokens:\n",
    "        if word in w2v_vectors:  # Check if word is in the Word2Vec model\n",
    "            vectors.append(w2v_vectors[word])  # Get the word vector\n",
    "        else:\n",
    "            vectors.append(np.zeros(300))  # Use a zero vector for words not in the model\n",
    "    if vectors:  \n",
    "        return np.mean(vectors, axis=0)  # Average the word vectors to represent the whole tweet\n",
    "    else:\n",
    "        return np.zeros(300)  # Use a zero vector for words not in the model\n",
    "\n",
    "\n",
    "# Generate word2vec representations\n",
    "x_train_vectors = np.array([tweet_to_avg_vector(tweet, w2v_vectors) for tweet in train_clean_tweets])\n",
    "x_test_vectors = np.array([tweet_to_avg_vector(tweet, w2v_vectors) for tweet in test_clean_tweets])\n",
    "\n",
    "# Labels for training and testing data\n",
    "label_train = temp_train_df[\"sentiment\"]\n",
    "label_test = temp_test_df[\"sentiment\"]\n",
    "\n",
    "# Train a LOGISTIC regression model (using word2vec vectors)\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(x_train_vectors, label_train)\n",
    "\n",
    "# Get predictions then evaluate performance on test set\n",
    "label_pred = lr.predict(x_test_vectors)\n",
    "print(classification_report(label_test, label_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70d2f786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.71      0.74     18967\n",
      "           1       0.80      0.85      0.82     26032\n",
      "\n",
      "    accuracy                           0.79     44999\n",
      "   macro avg       0.79      0.78      0.78     44999\n",
      "weighted avg       0.79      0.79      0.79     44999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression on large-twitter-tweets dataset using TF-idf vectors\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Convert tokenized tweets into strings that tf-idf vectorizer can actually use\n",
    "train_tweets_clean_joined = [' '.join(tokens) for tokens in train_clean_tweets]\n",
    "test_tweets_clean_joined = [' '.join(tokens) for tokens in test_clean_tweets]\n",
    "\n",
    "train_tweets_tfidf = vectorizer.fit_transform(train_tweets_clean_joined)\n",
    "test_tweets_tfidf = vectorizer.transform(test_tweets_clean_joined)\n",
    "\n",
    "\n",
    "# Train a multinomial logistic regression model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(train_tweets_tfidf, label_train)\n",
    "\n",
    "# Get predictions then evaluate performance on test set\n",
    "label_pred = lr.predict(test_tweets_tfidf)\n",
    "print(classification_report(label_test, label_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d20f48",
   "metadata": {},
   "source": [
    "### TF-idf or word2vec (large-twitter-tweets-sentiment)?\n",
    "\n",
    "#### Word2vec\n",
    "There are more positive tweets (104,125 in training, 26,032 in testing) than negative tweets (75,860 in training, 18,967 in testing) in the dataset, and that's reflected in the logistic regression classifier's performance. Negative tweets had an F1 score of 0.68, while positive tweets had an F1 score of 0.79. The macro and micro averages are the same though, both sitting at 0.74.\n",
    "\n",
    "#### TF-idf\n",
    "Again, the F1 score for the negative class (0.74) is lower than the F1 score for the positive class (0.82). In this case, the macro and micro averages were very similar with a macro average of 0.78 and a micro average of 0.79.\n",
    "\n",
    "\n",
    "Comparing the performance of the two models (logstic regression trained with word2vec vs tf-idf vectors), the TF-idf-trained model performed slightly better in this case, and it's faster, so I will be sticking with the tf-idf logistic regression model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bf6b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "\n",
    "# Create function to convert numerical sentiment labels into text\n",
    "def map_sentiment(value):\n",
    "    if value == 4:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "twitter_df[\"sentiment_num_conv\"] = twitter_df[\"sentiment\"].apply(map_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b07f6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.77    800000\n",
      "           1       0.75      0.84      0.79    800000\n",
      "\n",
      "    accuracy                           0.78   1600000\n",
      "   macro avg       0.79      0.78      0.78   1600000\n",
      "weighted avg       0.79      0.78      0.78   1600000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a random sample of 1000 rows to test code without taking forever\n",
    "# corpus_sample = twitter_df.sample(n=1000) \n",
    "\n",
    "\n",
    "####\n",
    "# Tokenize tweet text\n",
    "tweet_text = twitter_df[\"text\"].values  \n",
    "# tweet_text = corpus_sample[\"text\"].values\n",
    "tweet_tokenized = [tokenizer.tokenize(tweet) for tweet in tweet_text]\n",
    "corpus_tweets_clean = [_remove_airline_tok(tokens) for tokens in tweet_tokenized]\n",
    "\n",
    "# Convert tokenized tweets into strings that tf-idf vectorizer can actually use\n",
    "corpus_tweets_clean_joined = [' '.join(tokens) for tokens in corpus_tweets_clean]\n",
    "####\n",
    "\n",
    "# train_tweets_tfidf = vectorizer.fit_transform(train_tweets_clean_joined)\n",
    "corpus_tweets_tfidf = vectorizer.transform(corpus_tweets_clean_joined)\n",
    "\n",
    "\n",
    "# Get predictions then evaluate performance on test set\n",
    "corpus_label_pred = lr.predict(corpus_tweets_tfidf)\n",
    "print(classification_report(twitter_df[\"sentiment_num_conv\"], corpus_label_pred))\n",
    "# print(classification_report(corpus_sample[\"sentiment_num_conv\"], corpus_label_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9bede",
   "metadata": {},
   "source": [
    "#### Research Questions\n",
    "\n",
    "- What’s the relationship between sentiment and popularity?\n",
    "\n",
    "- Potential sub-question: Which entities have the most positive or negative sentiment, on average? (bert)\n",
    "\n",
    "- What are the most common entities in tweets associated with different sentiment labels?\n",
    "\n",
    "- What types of entities are the most common? Which entity types are more associated with each sentiment label? \n",
    "\n",
    "- What's the relationship between length of tweets and their average sentiment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01d088d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.77    800000\n",
      "           1       0.75      0.84      0.79    800000\n",
      "\n",
      "    accuracy                           0.78   1600000\n",
      "   macro avg       0.79      0.78      0.78   1600000\n",
      "weighted avg       0.79      0.78      0.78   1600000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####\n",
    "# Tokenize tweet text\n",
    "tweet_text = twitter_df[\"text\"].values  \n",
    "# tweet_text = corpus_sample[\"text\"].values\n",
    "tweet_tokenized = [tokenizer.tokenize(tweet) for tweet in tweet_text]\n",
    "corpus_tweets_clean = [_remove_airline_tok(tokens) for tokens in tweet_tokenized]\n",
    "\n",
    "# Convert tokenized tweets into strings that tf-idf vectorizer can actually use\n",
    "corpus_tweets_clean_joined = [' '.join(tokens) for tokens in corpus_tweets_clean]\n",
    "####\n",
    "\n",
    "# train_tweets_tfidf = vectorizer.fit_transform(train_tweets_clean_joined)\n",
    "corpus_tweets_tfidf = vectorizer.transform(corpus_tweets_clean_joined)\n",
    "\n",
    "\n",
    "# Get predictions then evaluate performance on test set\n",
    "corpus_label_pred = lr.predict(corpus_tweets_tfidf)\n",
    "print(classification_report(twitter_df[\"sentiment_num_conv\"], corpus_label_pred))\n",
    "# print(classification_report(corpus_sample[\"sentiment_num_conv\"], corpus_label_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c7e1c",
   "metadata": {},
   "source": [
    "### What’s the relationship between sentiment and popularity???\n",
    "Based on the top 10 most common spans, there doesn't really seem to be a relationship! If a label of 0 = negative sentiment and a 1 = positive sentiment, then on average, entities that appear in more positive tweets would have a sentiment closer to 1. Lookin at the top 10 most common spans and their average sentiments, we get this:\n",
    "\n",
    "Top 10 most common entity spans and their average sentiment (logistic regression prediction):\n",
    "  - today: 57597 instances || Average sentiment = 0.46\n",
    "  - tomorrow: 27349 instances || Average sentiment = 0.45\n",
    "  - 2: 26003 instances || Average sentiment = 0.49\n",
    "  - tonight: 23124 instances || Average sentiment = 0.52\n",
    "  - one: 14757 instances || Average sentiment = 0.6\n",
    "  - first: 12105 instances || Average sentiment = 0.69\n",
    "  - 4: 9864 instances || Average sentiment = 0.55\n",
    "  - last night: 9733 instances || Average sentiment = 0.43\n",
    "  - morning: 8907 instances || Average sentiment = 0.78\n",
    "  - yesterday: 8535 instances || Average sentiment = 0.43\n",
    "\n",
    "Top 10 most common entity spans and their average sentiment (twitter_df ground truth):\n",
    "  - today: 57597 instances || Average sentiment = 0.45\n",
    "  - tomorrow: 27349 instances || Average sentiment = 0.44\n",
    "  - 2: 26003 instances || Average sentiment = 0.46\n",
    "  - tonight: 23124 instances || Average sentiment = 0.5\n",
    "  - one: 14757 instances || Average sentiment = 0.56\n",
    "  - first: 12105 instances || Average sentiment = 0.64\n",
    "  - 4: 9864 instances || Average sentiment = 0.52\n",
    "  - last night: 9733 instances || Average sentiment = 0.41\n",
    "  - morning: 8907 instances || Average sentiment = 0.72\n",
    "  - yesterday: 8535 instances || Average sentiment = 0.42\n",
    "\n",
    "It looks like, aside from \"one,\" \"first,\" and \"morning,\" the ten most common entity spans generally have average sentiment scores around 0.5 or slightly below it! It seems that an entity being more common or popular doesn't make it related to more positive or negative discussions. Another note is that the average sentiments generated from the sentiment predictions from the logistic regression model tend to be a bit higher than the averages generated from ground truth sentiment labels for the top 10 most common entities (ex: \"morning\" has an average sentiment of 0.78 according to the logistic regression labels, but a 0.72 based on ground truth sentiment labels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c7cdf927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common entity spans:\n",
      "  - today: 57597 instances\n",
      "  - tomorrow: 27349 instances\n",
      "  - 2: 26003 instances\n",
      "  - tonight: 23124 instances\n",
      "  - one: 14757 instances\n",
      "  - first: 12105 instances\n",
      "  - 4: 9864 instances\n",
      "  - last night: 9733 instances\n",
      "  - morning: 8907 instances\n",
      "  - yesterday: 8535 instances\n"
     ]
    }
   ],
   "source": [
    "# Initialize entity span counter\n",
    "entity_spans = Counter()\n",
    "\n",
    "# Iterate over each tweet and count spans\n",
    "for doc in doc_web:\n",
    "    for ent in doc.ents:\n",
    "        entity_spans.update([ent.text])  \n",
    "\n",
    "# Get top 10 most common spans\n",
    "most_common_ents = entity_spans.most_common(10)\n",
    "\n",
    "# Print the top 10 most common spans with their counts\n",
    "print(\"Top 10 most common entity spans:\")\n",
    "for span, count in most_common_ents:\n",
    "    print(f\"  - {span}: {count} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89b72e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common entity spans and their average sentiment (logistic regression prediction):\n",
      "  - today: 57597 instances || Average sentiment = 0.46\n",
      "  - tomorrow: 27349 instances || Average sentiment = 0.45\n",
      "  - 2: 26003 instances || Average sentiment = 0.49\n",
      "  - tonight: 23124 instances || Average sentiment = 0.52\n",
      "  - one: 14757 instances || Average sentiment = 0.6\n",
      "  - first: 12105 instances || Average sentiment = 0.69\n",
      "  - 4: 9864 instances || Average sentiment = 0.55\n",
      "  - last night: 9733 instances || Average sentiment = 0.43\n",
      "  - morning: 8907 instances || Average sentiment = 0.78\n",
      "  - yesterday: 8535 instances || Average sentiment = 0.43\n",
      "\n",
      "Top 10 most common entity spans and their average sentiment (twitter_df ground truth):\n",
      "  - today: 57597 instances || Average sentiment = 0.45\n",
      "  - tomorrow: 27349 instances || Average sentiment = 0.44\n",
      "  - 2: 26003 instances || Average sentiment = 0.46\n",
      "  - tonight: 23124 instances || Average sentiment = 0.5\n",
      "  - one: 14757 instances || Average sentiment = 0.56\n",
      "  - first: 12105 instances || Average sentiment = 0.64\n",
      "  - 4: 9864 instances || Average sentiment = 0.52\n",
      "  - last night: 9733 instances || Average sentiment = 0.41\n",
      "  - morning: 8907 instances || Average sentiment = 0.72\n",
      "  - yesterday: 8535 instances || Average sentiment = 0.42\n"
     ]
    }
   ],
   "source": [
    "# Get the average sentiment associated with each of the top 10 most common spans\n",
    "\n",
    "# Initialize entity span counter and dictionary to track sentiment for each entity\n",
    "# entity_spans = Counter() COMM\n",
    "entity_sentiments = {}\n",
    "\n",
    "# Method for iterating through doc web and finding average of its sentiment across all tweets\n",
    "# Parameter \"label_source\" defines if you want the to use sentiment labels predicted by model, or the ground truth\n",
    "# from twitter_df\n",
    "def get_avg_sents(label_source):\n",
    "    # Connect each tweet in doc_web with its predicted sentiment\n",
    "    for i, (doc, sentiment) in enumerate(zip(doc_web, label_source)): \n",
    "        for ent in doc.ents:\n",
    "            \n",
    "            # Store sentiment for the entity in the dictionary\n",
    "            if ent.text not in entity_sentiments:\n",
    "                entity_sentiments[ent.text] = []  # Create list for the entity\n",
    "            entity_sentiments[ent.text].append(sentiment)  # Add sentiments to list\n",
    "\n",
    "\n",
    "    for span, count in most_common_ents:\n",
    "        avg_sentiment = np.mean(entity_sentiments[span])  # Average of sentiment values\n",
    "\n",
    "        # Silly but helpful resource: https://www.geeksforgeeks.org/round-function-python/\n",
    "        print(f\"  - {span}: {count} instances || Average sentiment = {round(avg_sentiment, 2)}\")\n",
    "\n",
    "print(\"Top 10 most common entity spans and their average sentiment (logistic regression prediction):\")\n",
    "get_avg_sents(corpus_label_pred)\n",
    "print()\n",
    "print(\"Top 10 most common entity spans and their average sentiment (twitter_df ground truth):\")\n",
    "get_avg_sents(twitter_df[\"sentiment_num_conv\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37ae32",
   "metadata": {},
   "source": [
    "#### Which entities have the most positive or negative sentiment, on average???\n",
    "\n",
    "Note: I decided to exclude entities that have an average sentiment score of exactly 0 (negative) or 1 (positive) to avoid tweets that only appear once in a negative or positive tweet from messing with the results.\n",
    "\n",
    "Looking at the 10 entities with the highest and lowest average sentiments, we can see that the labels created by the logistic regression model and the ground truth labels that came with the corpus give us different results. However, there are a few commonalities. For the positive entities, variations on \"FollowFriday\" appear both logistic-regression and ground-truth label groups. For negative entities, spansl like \"F*CK*\" and \"Boo Hoo\" appear in the top 10 negative entities lists for both label sources as well. \n",
    "\n",
    "For the logistic regression group, it seems that the top 10 negative entities are WAY more common than their positive counterparts on average (129.9 tweets on avg for negative vs 50.4 for positive). This might be explained by the extremely high number of tweets that the negative span \"UGH\" appears in (724 different tweets). However, it's different for the ground truth labels, where the average number of tweets the top 10 most positive entities occur in is slightly higher than the negative side (32.1 vs 26.8 for negative).\n",
    "\n",
    "The most interesting thing I observed here: I'm surprised \"UGH\" isn't incldued as one of the top 10 negative entities for the ground-truth labels. That's a very negative term!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "613d0fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities with the most positive and negative sentiment (logistic regression prediction):\n",
      "\n",
      "Top 10 Most Positive Entities (excluding sentiment = 0 or 1):\n",
      "  - Sweet Dreams: 84 tweets || Average Sentiment = 0.99\n",
      "  - #followfriday: 78 tweets || Average Sentiment = 0.99\n",
      "  - Ð: 55 tweets || Average Sentiment = 0.98\n",
      "  - followfriday: 54 tweets || Average Sentiment = 0.98\n",
      "  - YUMMY: 48 tweets || Average Sentiment = 0.98\n",
      "  - ÐºÐ: 43 tweets || Average Sentiment = 0.98\n",
      "  - #ff: 39 tweets || Average Sentiment = 0.97\n",
      "  - Yum: 36 tweets || Average Sentiment = 0.97\n",
      "  - VINES: 34 tweets || Average Sentiment = 0.97\n",
      "  - @joelmadden: 33 tweets || Average Sentiment = 0.97\n",
      "\n",
      "Average Number of Tweets for Top 10 Positive Entities: 50.4\n",
      "\n",
      "Top 10 Most Negative Entities (excluding sentiment = 0 or 1):\n",
      "  - RIP Farrah: 29 tweets || Average Sentiment = 0.03\n",
      "  - UGH: 724 tweets || Average Sentiment = 0.03\n",
      "  - Boo hoo: 32 tweets || Average Sentiment = 0.03\n",
      "  - worst day: 32 tweets || Average Sentiment = 0.03\n",
      "  - eurgh: 33 tweets || Average Sentiment = 0.03\n",
      "  - a bad week: 35 tweets || Average Sentiment = 0.03\n",
      "  - F**K: 38 tweets || Average Sentiment = 0.03\n",
      "  - OUCH: 72 tweets || Average Sentiment = 0.01\n",
      "  - a sad day: 150 tweets || Average Sentiment = 0.01\n",
      "  - ughhh: 154 tweets || Average Sentiment = 0.01\n",
      "\n",
      "Average Number of Tweets for Top 10 Negative Entities: 129.9\n",
      "\n",
      "Entities with the most positive and negative sentiment (twitter_df ground truth):\n",
      "\n",
      "Top 10 Most Positive Entities (excluding sentiment = 0 or 1):\n",
      "  - #delongeday: 17 tweets || Average Sentiment = 1.0\n",
      "  - #followfriday: 78 tweets || Average Sentiment = 0.99\n",
      "  - FollowFriday: 26 tweets || Average Sentiment = 0.98\n",
      "  - Woot Woot: 25 tweets || Average Sentiment = 0.98\n",
      "  - #FollowFriday: 24 tweets || Average Sentiment = 0.98\n",
      "  - http://bit.ly/PmvRY: 20 tweets || Average Sentiment = 0.98\n",
      "  - Debby: 20 tweets || Average Sentiment = 0.98\n",
      "  - #ff: 39 tweets || Average Sentiment = 0.97\n",
      "  - followfriday: 54 tweets || Average Sentiment = 0.97\n",
      "  - hcb: 18 tweets || Average Sentiment = 0.97\n",
      "\n",
      "Average Number of Tweets for Top 10 Positive Entities: 32.1\n",
      "\n",
      "Top 10 Most Negative Entities (excluding sentiment = 0 or 1):\n",
      "  - F**K: 38 tweets || Average Sentiment = 0.03\n",
      "  - Gif: 19 tweets || Average Sentiment = 0.03\n",
      "  - JUM: 19 tweets || Average Sentiment = 0.03\n",
      "  - ache: 21 tweets || Average Sentiment = 0.02\n",
      "  - boo hoo: 24 tweets || Average Sentiment = 0.02\n",
      "  - ow: 21 tweets || Average Sentiment = 0.02\n",
      "  - RIP David Carradine: 28 tweets || Average Sentiment = 0.02\n",
      "  - RIP Farrah: 29 tweets || Average Sentiment = 0.02\n",
      "  - Boo hoo: 32 tweets || Average Sentiment = 0.02\n",
      "  - Booo: 37 tweets || Average Sentiment = 0.01\n",
      "\n",
      "Average Number of Tweets for Top 10 Negative Entities: 26.8\n"
     ]
    }
   ],
   "source": [
    "# Initialize counter to track how many tweets an entity appears in\n",
    "entity_doc_count = Counter()\n",
    "\n",
    "# Iterate over each document and count spans \n",
    "for doc in doc_web:\n",
    "    # Use set to make sure each entity only gets counted once per tweet\n",
    "    seen_entities = set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.text not in seen_entities:\n",
    "            entity_doc_count.update([ent.text])  # Entity appears in this tweet\n",
    "            seen_entities.add(ent.text)  # Mark entity as seen (in this tweet)\n",
    "\n",
    "# Get top 10 most common spans from the filtered entities\n",
    "items = entity_doc_count.items()\n",
    "sorted_items = sorted(items, key=lambda x: x[1], reverse=True)\n",
    "most_common_ents = sorted_items[:10]\n",
    "\n",
    "# Initialize dictionary to track sentiment for each entity\n",
    "entity_sentiments = {}\n",
    "\n",
    "# Method to calculate average sentiment for entities and determine most positive/negative ones\n",
    "def get_avg_sents(label_source):\n",
    "    # Connect each tweet in doc_web with its predicted sentiment\n",
    "    for i, (doc, sentiment) in enumerate(zip(doc_web, label_source)): \n",
    "        for ent in doc.ents:\n",
    "            if ent.text in entity_doc_count:\n",
    "                \n",
    "                # Store sentiment for the entity in the dictionary\n",
    "                if ent.text not in entity_sentiments:\n",
    "                    entity_sentiments[ent.text] = []  # Initialize the list for this entity\n",
    "                entity_sentiments[ent.text].append(sentiment)  # Append sentiment value\n",
    "\n",
    "    # Get avg sentiment for the filtered entities\n",
    "    entity_avg_sentiments = {}\n",
    "    for entity, sentiments in entity_sentiments.items():\n",
    "        avg_sentiment = np.mean(sentiments)  # Calculate the mean sentiment for each entity\n",
    "        entity_avg_sentiments[entity] = avg_sentiment  # Store the average sentiment\n",
    "\n",
    "    # Filter out entities where avg sentiment = 0 or = 1\n",
    "    filtered_avg_sentiments = {entity: avg_sentiment for entity, avg_sentiment in entity_avg_sentiments.items()\n",
    "                               if avg_sentiment != 0 and avg_sentiment != 1}\n",
    "\n",
    "    # Sort entities by avg sentiment (highest to lowest)\n",
    "    items = filtered_avg_sentiments.items()\n",
    "    sorted_entities = sorted(items, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    \n",
    "    print(\"\\nTop 10 Most Positive Entities (excluding sentiment = 0 or 1):\")\n",
    "    sum_pos_tweets = 0\n",
    "    for entity, avg_sentiment in sorted_entities[:10]:  # Top 10 most positive\n",
    "        tweet_count = entity_doc_count[entity]\n",
    "        sum_pos_tweets += tweet_count\n",
    "        print(f\"  - {entity}: {tweet_count} tweets || Average Sentiment = {round(avg_sentiment, 2)}\")\n",
    "        \n",
    "    avg_positive_tweets = sum_pos_tweets / 10  # Calculate average tweet count\n",
    "    print(f\"\\nAverage Number of Tweets for Top 10 Positive Entities: {round(avg_positive_tweets, 2)}\")\n",
    "\n",
    "    print(\"\\nTop 10 Most Negative Entities (excluding sentiment = 0 or 1):\")\n",
    "    sum_neg_tweets = 0\n",
    "    for entity, avg_sentiment in sorted_entities[-10:]:  # Bottom 10 most negative\n",
    "        tweet_count = entity_doc_count[entity]\n",
    "        sum_neg_tweets += tweet_count\n",
    "        print(f\"  - {entity}: {tweet_count} tweets || Average Sentiment = {round(avg_sentiment, 2)}\")\n",
    "    \n",
    "    avg_neg_tweets = sum_neg_tweets / 10  # Calculate average tweet count\n",
    "    print(f\"\\nAverage Number of Tweets for Top 10 Negative Entities: {round(avg_neg_tweets, 2)}\")\n",
    "\n",
    "\n",
    "# Run the function to get the entities with the most positive and negative sentiment\n",
    "print(\"Entities with the most positive and negative sentiment (logistic regression prediction):\")\n",
    "get_avg_sents(corpus_label_pred)\n",
    "print()\n",
    "print(\"Entities with the most positive and negative sentiment (twitter_df ground truth):\")\n",
    "get_avg_sents(twitter_df[\"sentiment_num_conv\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589854a0",
   "metadata": {},
   "source": [
    "### What are the most common entities in tweets associated with different sentiment labels???\n",
    "\n",
    "It seems that across both positive and negative tweets, regardless of which label source we're using (logistic regression prediction vs ground truth), the same spans are the most common across the board! \"Today\" is the most commonly occurring entity across tweets of all sentiments and label sources, and other entities like \"2\", \"tomorrow\", and \"tonight\" are very common as well. However, there are some entities that only appear in the top 10 frequency list for either the positive or sentiment label class. For example \"LOL\" only appears in the top 10 list for entities with positive sentiment, while \"yesterday\" only appears in the otp 10 list for entities with negative sentiment. The top 10 entities for each sentiment label are pretty consistent aross both the logistic regression labels and ground truth labels, so it seems that the most common entities are so common that the differences in label performance don't really make a difference in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6fee049f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Prediction Labels:\n",
      "\n",
      "Top 10 Most Common Entity Spans (Positive Sentiment):\n",
      "  - today: 26730 instances\n",
      "  - 2: 12816 instances\n",
      "  - tomorrow: 12306 instances\n",
      "  - tonight: 11916 instances\n",
      "  - one: 8908 instances\n",
      "  - first: 8365 instances\n",
      "  - morning: 6950 instances\n",
      "  - LOL: 6149 instances\n",
      "  - #: 5633 instances\n",
      "  - 4: 5469 instances\n",
      "\n",
      "Top 10 Most Common Entity Spans (Negative Sentiment):\n",
      "  - today: 30867 instances\n",
      "  - tomorrow: 15043 instances\n",
      "  - 2: 13187 instances\n",
      "  - tonight: 11208 instances\n",
      "  - one: 5849 instances\n",
      "  - last night: 5558 instances\n",
      "  - yesterday: 4900 instances\n",
      "  - 4: 4395 instances\n",
      "  - 3: 4268 instances\n",
      "  - first: 3740 instances\n",
      "\n",
      "Ground Truth Sentiment Labels:\n",
      "\n",
      "Top 10 Most Common Entity Spans (Positive Sentiment):\n",
      "  - today: 24854 instances\n",
      "  - tomorrow: 11929 instances\n",
      "  - 2: 11115 instances\n",
      "  - tonight: 11015 instances\n",
      "  - one: 7548 instances\n",
      "  - first: 7168 instances\n",
      "  - morning: 5845 instances\n",
      "  - LOL: 5597 instances\n",
      "  - #: 4906 instances\n",
      "  - 4: 4864 instances\n",
      "\n",
      "Top 10 Most Common Entity Spans (Negative Sentiment):\n",
      "  - today: 32743 instances\n",
      "  - tomorrow: 15420 instances\n",
      "  - 2: 14888 instances\n",
      "  - tonight: 12109 instances\n",
      "  - one: 7209 instances\n",
      "  - last night: 5949 instances\n",
      "  - 4: 5000 instances\n",
      "  - first: 4937 instances\n",
      "  - yesterday: 4931 instances\n",
      "  - 3: 4678 instances\n"
     ]
    }
   ],
   "source": [
    "# Splitting doc_web by sentiment and counting by spans\n",
    "def count_entity_spans_by_sentiment(doc_web, label_source):\n",
    "    \n",
    "    # Initialize positive and negative entity counters\n",
    "    positive_entity_spans = Counter()\n",
    "    negative_entity_spans = Counter()\n",
    "\n",
    "    # Iterate over each document + sentiment label\n",
    "    for doc, sentiment in zip(doc_web, label_source):\n",
    "        if sentiment == 1: # Positive\n",
    "            for ent in doc.ents:\n",
    "                positive_entity_spans.update([ent.text])\n",
    "        elif sentiment == 0:\n",
    "            for ent in doc.ents:\n",
    "                negative_entity_spans.update([ent.text])\n",
    "    \n",
    "    # Get the most common entity spans for positive and negative sentiments\n",
    "    print(\"\\nTop 10 Most Common Entity Spans (Positive Sentiment):\")\n",
    "    most_common_positive_ents = positive_entity_spans.most_common(10)\n",
    "    for span, count in most_common_positive_ents:\n",
    "        print(f\"  - {span}: {count} instances\")\n",
    "    \n",
    "    print(\"\\nTop 10 Most Common Entity Spans (Negative Sentiment):\")\n",
    "    most_common_negative_ents = negative_entity_spans.most_common(10)\n",
    "    for span, count in most_common_negative_ents:\n",
    "        print(f\"  - {span}: {count} instances\")\n",
    "\n",
    "print(\"Logistic Regression Prediction Labels:\")\n",
    "count_entity_spans_by_sentiment(doc_web, corpus_label_pred)\n",
    "print()\n",
    "print(\"Ground Truth Sentiment Labels:\")\n",
    "count_entity_spans_by_sentiment(doc_web, twitter_df[\"sentiment_num_conv\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a0603d",
   "metadata": {},
   "source": [
    "#### What types of entities are the most common? Which entity types are more associated with each sentiment label???\n",
    "\n",
    "Across both the predicted and ground truth labels, the top three most common entity types for positive tweets are \"PERSON\", \"ORG\", and \"DATE\". The top three most common entity types for negative tweets are \"DATE\", \"PERSON\", and \"ORG\". It's interesting that the top entity type in positive tweets is \"PERSON,\" but the most common entity type in negative tweets is \"DATE\"! \n",
    "\n",
    "The 4th-9th most common entity types is the same across both label classes as well, but the 10th most common entity type changed. For positive tweets, it was \"WORK_OF_ART,\" while for negative tweets the 10th most common entity type was \"MONEY\". IT seems that people are happier when they tweet about works of art than they are when they tweet abount money!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df66166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON         344579.0\n",
      "DATE           291742.0\n",
      "ORG            305084.0\n",
      "PERCENT          2850.0\n",
      "ORDINAL         26742.0\n",
      "CARDINAL       158404.0\n",
      "LOC             11457.0\n",
      "TIME           114966.0\n",
      "GPE            138184.0\n",
      "NORP            36439.0\n",
      "QUANTITY         5938.0\n",
      "WORK_OF_ART     12484.0\n",
      "FAC              8745.0\n",
      "MONEY           13660.0\n",
      "PRODUCT         16398.0\n",
      "LANGUAGE         3032.0\n",
      "EVENT            2842.0\n",
      "LAW              1487.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Function to count entity types in a single document\n",
    "def count_entity_types(doc):\n",
    "    entity_types = [ent.label_ for ent in doc.ents]\n",
    "    return Counter(entity_types)\n",
    "\n",
    "entity_counts = doc_web.apply(count_entity_types)\n",
    "\n",
    "# Convert to dataframe to make summarizing easier\n",
    "entity_counts_df = pd.DataFrame(list(entity_counts))\n",
    "\n",
    "# Find the sum of instances for each entity type\n",
    "sum_entity_counts = entity_counts_df.sum()\n",
    "\n",
    "print(sum_entity_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d230e817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Prediction Labels:\n",
      "\n",
      "Top 10 Most Common Entity Types (Positive Sentiment):\n",
      "  - PERSON: 218269 instances\n",
      "  - ORG: 187922 instances\n",
      "  - DATE: 145081 instances\n",
      "  - CARDINAL: 87313 instances\n",
      "  - GPE: 82267 instances\n",
      "  - TIME: 58310 instances\n",
      "  - NORP: 23362 instances\n",
      "  - ORDINAL: 17113 instances\n",
      "  - PRODUCT: 10401 instances\n",
      "  - WORK_OF_ART: 9423 instances\n",
      "\n",
      "Top 10 Most Common Entity Types (Negative Sentiment):\n",
      "  - DATE: 146661 instances\n",
      "  - PERSON: 126310 instances\n",
      "  - ORG: 117162 instances\n",
      "  - CARDINAL: 71091 instances\n",
      "  - TIME: 56656 instances\n",
      "  - GPE: 55917 instances\n",
      "  - NORP: 13077 instances\n",
      "  - ORDINAL: 9629 instances\n",
      "  - PRODUCT: 5997 instances\n",
      "  - MONEY: 5550 instances\n",
      "\n",
      "Ground Truth Sentiment Labels:\n",
      "\n",
      "Top 10 Most Common Entity Types (Positive Sentiment):\n",
      "  - PERSON: 194550 instances\n",
      "  - ORG: 168014 instances\n",
      "  - DATE: 133217 instances\n",
      "  - CARDINAL: 75044 instances\n",
      "  - GPE: 71499 instances\n",
      "  - TIME: 52307 instances\n",
      "  - NORP: 20339 instances\n",
      "  - ORDINAL: 14728 instances\n",
      "  - PRODUCT: 9186 instances\n",
      "  - WORK_OF_ART: 8400 instances\n",
      "\n",
      "Top 10 Most Common Entity Types (Negative Sentiment):\n",
      "  - DATE: 158525 instances\n",
      "  - PERSON: 150029 instances\n",
      "  - ORG: 137070 instances\n",
      "  - CARDINAL: 83360 instances\n",
      "  - GPE: 66685 instances\n",
      "  - TIME: 62659 instances\n",
      "  - NORP: 16100 instances\n",
      "  - ORDINAL: 12014 instances\n",
      "  - PRODUCT: 7212 instances\n",
      "  - MONEY: 6843 instances\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Splitting doc_web by sentiment and counting by entity types\n",
    "def count_entity_types_by_sentiment(doc_web, label_source):\n",
    "    \n",
    "    # Initialize positive and negative entity type counters\n",
    "    positive_entity_types = Counter()\n",
    "    negative_entity_types = Counter()\n",
    "\n",
    "    # Iterate over each document + sentiment label\n",
    "    for doc, sentiment in zip(doc_web, label_source):\n",
    "        if sentiment == 1:  # Positive\n",
    "            for ent in doc.ents:\n",
    "                positive_entity_types.update([ent.label_])  # Use entity type\n",
    "        elif sentiment == 0:  # Negative\n",
    "            for ent in doc.ents:\n",
    "                negative_entity_types.update([ent.label_])  # Use entity type\n",
    "    \n",
    "    # Get the most common entity types for positive and negative sentiments\n",
    "    print(\"\\nTop 10 Most Common Entity Types (Positive Sentiment):\")\n",
    "    most_common_positive_types = positive_entity_types.most_common(10)\n",
    "    for entity_type, count in most_common_positive_types:\n",
    "        print(f\"  - {entity_type}: {count} instances\")\n",
    "    \n",
    "    print(\"\\nTop 10 Most Common Entity Types (Negative Sentiment):\")\n",
    "    most_common_negative_types = negative_entity_types.most_common(10)\n",
    "    for entity_type, count in most_common_negative_types:\n",
    "        print(f\"  - {entity_type}: {count} instances\")\n",
    "\n",
    "print(\"Logistic Regression Prediction Labels:\")\n",
    "count_entity_types_by_sentiment(doc_web, corpus_label_pred)\n",
    "print()\n",
    "print(\"Ground Truth Sentiment Labels:\")\n",
    "count_entity_types_by_sentiment(doc_web, twitter_df[\"sentiment_num_conv\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f98d164",
   "metadata": {},
   "source": [
    "### What's the relationship between length of tweets and their average sentiment?\n",
    "It seems that there's some fluctuation around the 20-40 range, but as the length of a tweet goes beyond 40 words, the average sentiment of the tweet starts to increase. Loooking at groud truth labels, the average sentiment for the shortest group is 0.51 and goes as high as 0.78 for tweets in the 80+ words range. That correlation is more pronounced when using predicted labels, as the average sentiments jump from 0.57 to as high as 0.95 between the shortest and longest tweet groups when assessing using predicted sentiment labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f1c95c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentiment by tweet length category (logistic predictions):\n",
      "length_category\n",
      "20 words or less    0.570754\n",
      "40 words or less    0.513586\n",
      "60 words or less    0.777950\n",
      "80 words or less    0.928030\n",
      "80 words+           0.950472\n",
      "Name: corpus_label_pred, dtype: float64\n",
      "\n",
      "Average sentiment by tweet length category (ground truth):\n",
      "length_category\n",
      "20 words or less    0.510553\n",
      "40 words or less    0.469878\n",
      "60 words or less    0.580745\n",
      "80 words or less    0.662879\n",
      "80 words+           0.783019\n",
      "Name: sentiment_num_conv, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate tweet lengths\n",
    "len_arr = [len(tweet) for tweet in clean_tweets]\n",
    "twitter_df['tweet_length'] = len_arr  # Add tweet length as a new column\n",
    "\n",
    "# Group tweets into different lengths\n",
    "def categorize_length(length):\n",
    "    if length <= 20:\n",
    "        return '20 words or less'\n",
    "    elif length <= 40:\n",
    "        return '40 words or less'\n",
    "    elif length <= 60:\n",
    "        return '60 words or less'\n",
    "    elif length <= 80:\n",
    "        return '80 words or less'\n",
    "    else:\n",
    "        return '80 words+'\n",
    "\n",
    "# Add a new column for length category for each tweet in twitter_df\n",
    "twitter_df['length_category'] = twitter_df['tweet_length'].apply(categorize_length)\n",
    "\n",
    "# Calculate average sentiment for each tweet length category\n",
    "twitter_df['corpus_label_pred'] = corpus_label_pred  # Add predictions as a column to make next steps easier\n",
    "avg_sentiment_by_length_pred = twitter_df.groupby('length_category')['corpus_label_pred'].mean()\n",
    "print(\"Average sentiment by tweet length category (logistic predictions):\")\n",
    "print(avg_sentiment_by_length_pred)\n",
    "\n",
    "print()\n",
    "\n",
    "avg_sentiment_by_length_truth = twitter_df.groupby('length_category')['sentiment_num_conv'].mean()\n",
    "print(\"Average sentiment by tweet length category (ground truth):\")\n",
    "print(avg_sentiment_by_length_truth)\n",
    "\n",
    "# # Step 2: Plot the sentiment by length category\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# avg_sentiment_by_length.plot(kind='bar', color='skyblue')\n",
    "# plt.title(\"Average Sentiment by Tweet Length Category\")\n",
    "# plt.xlabel(\"Tweet Length Category\")\n",
    "# plt.ylabel(\"Average Sentiment\")\n",
    "# plt.xticks(rotation=0)\n",
    "# plt.show()\n",
    "\n",
    "# # Step 3: If you want to analyze by specific tweet lengths (continuous variable)\n",
    "# # Calculate average sentiment for each tweet length (continuous)\n",
    "# avg_sentiment_by_length_continuous = twitter_df.groupby('tweet_length')['sentiment'].mean()\n",
    "\n",
    "# # Plot sentiment change based on tweet length (continuous)\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# avg_sentiment_by_length_continuous.plot(kind='line', color='orange')\n",
    "# plt.title(\"Average Sentiment by Tweet Length (Continuous)\")\n",
    "# plt.xlabel(\"Tweet Length\")\n",
    "# plt.ylabel(\"Average Sentiment\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c083efe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
