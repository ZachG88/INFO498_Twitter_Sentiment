{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1779bab-7e81-4a7e-a730-4804fd8d4fc5",
   "metadata": {},
   "source": [
    "## 498 Final - Model and Implementation (Zach Gendreau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cba491b8-3b1d-4238-9d78-f1f5e8002698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in training dataset (Airline sentiment tweet data)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "airline_df = pd.read_csv(\"hf://datasets/osanseviero/twitter-airline-sentiment/Tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e6d45a-46cb-42ea-ac48-a17cf9e8c1a4",
   "metadata": {},
   "source": [
    "### Dataset Description: large-twitter-tweets-sentiment\n",
    "Pulled from Huggingface (link: https://huggingface.co/datasets/gxb912/large-twitter-tweets-sentiment), there are 224,994 tweets in it total, with 179,995 tweets in the training split and 44,999 tweets in the testing split. The dataset didn't include any data aside from the text and sentiment label of each tweet. There was no other information on the source of the dataset aside from the fact that it was annotated specifically for sentiment analysis.\n",
    "\n",
    "In the training split, there are 104,125 positive tweets, and 75,860 negative tweets. In the test split, there are 26,032 positive tweets, and 18,967 negative tweets. This is an uneven spread with more positive tweets, but nothing too extreme. There average length of tweets in both the training and testing data is about ~15 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66cde9e3-56d7-4944-a796-bd0141e0a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train_df = pd.read_csv(\"hf://datasets/gxb912/large-twitter-tweets-sentiment/train.csv\")\n",
    "temp_test_df = pd.read_csv(\"hf://datasets/gxb912/large-twitter-tweets-sentiment/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91558835-34f7-460f-98b5-17a092f6bbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (179995, 2)\n",
      "Testing: (44999, 2)\n",
      "\n",
      "Sentiment frequency\n",
      "Training data:\n",
      "sentiment\n",
      "1    104125\n",
      "0     75870\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Testing data:\n",
      "sentiment\n",
      "1    26032\n",
      "0    18967\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Tokenize tweet text (training)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m tweet_text \u001b[38;5;241m=\u001b[39m temp_train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 25\u001b[0m tweet_tokenized \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mtokenize(tweet) \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweet_text]\n\u001b[1;32m     26\u001b[0m train_clean_tweets \u001b[38;5;241m=\u001b[39m [_remove_airline_tok(tokens) \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m tweet_tokenized]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Tokenize tweet text (testing)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Tokenize tweet text (training)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m tweet_text \u001b[38;5;241m=\u001b[39m temp_train_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 25\u001b[0m tweet_tokenized \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mtokenize(tweet) \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweet_text]\n\u001b[1;32m     26\u001b[0m train_clean_tweets \u001b[38;5;241m=\u001b[39m [_remove_airline_tok(tokens) \u001b[38;5;28;01mfor\u001b[39;00m tokens \u001b[38;5;129;01min\u001b[39;00m tweet_tokenized]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Tokenize tweet text (testing)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/nltk/tokenize/casual.py:380\u001b[0m, in \u001b[0;36mTweetTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# Recognise phone numbers during tokenization\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch_phone_numbers:\n\u001b[0;32m--> 380\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPHONE_WORD_RE\u001b[38;5;241m.\u001b[39mfindall(safe_text)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    382\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWORD_RE\u001b[38;5;241m.\u001b[39mfindall(safe_text)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 0 = negative, 1 = positive\n",
    "# load twitter dataset into pandas \n",
    "from nltk.tokenize import TweetTokenizer \n",
    "\n",
    "print(\"Training: \" + str(temp_train_df.shape))\n",
    "print(\"Testing: \" + str(temp_test_df.shape))\n",
    "print()\n",
    "\n",
    "# Get sentiment counts\n",
    "print(\"Sentiment frequency\")\n",
    "print(\"Training data:\")\n",
    "print(temp_train_df['sentiment'].value_counts())\n",
    "print() \n",
    "print(\"Testing data:\")\n",
    "print(temp_test_df['sentiment'].value_counts())\n",
    "\n",
    "# Method for removing the \"@airline\" token from a tweet\n",
    "def _remove_airline_tok(tokens):\n",
    "    return tokens[1:] if tokens[0].startswith('@') else tokens   \n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "# Tokenize tweet text (training)\n",
    "tweet_text = temp_train_df[\"text\"].values\n",
    "tweet_tokenized = [tokenizer.tokenize(tweet) for tweet in tweet_text]\n",
    "train_clean_tweets = [_remove_airline_tok(tokens) for tokens in tweet_tokenized]\n",
    "\n",
    "# Tokenize tweet text (testing)\n",
    "tweet_text = temp_test_df[\"text\"].values\n",
    "tweet_tokenized = [tokenizer.tokenize(tweet) for tweet in tweet_text]\n",
    "test_clean_tweets = [_remove_airline_tok(tokens) for tokens in tweet_tokenized]\n",
    "    \n",
    "# Find the average length of tweets \n",
    "def average_tweet_length_finder(df):\n",
    "    len_arr = []\n",
    "    for tweet in df:\n",
    "        tweet_length = len(tweet)\n",
    "        len_arr.append(tweet_length)\n",
    "    tweet_mean = np.mean(len_arr)\n",
    "    print(\"Average length of tweets:\", tweet_mean)\n",
    "    \n",
    "print(\"Training:\")\n",
    "average_tweet_length_finder(train_clean_tweets)\n",
    "\n",
    "print(\"Testing:\")\n",
    "average_tweet_length_finder(test_clean_tweets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f624105-0f9c-4e66-b3d4-6bd85007bff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.64      0.68     18967\n",
      "           1       0.76      0.82      0.79     26032\n",
      "\n",
      "    accuracy                           0.75     44999\n",
      "   macro avg       0.74      0.73      0.74     44999\n",
      "weighted avg       0.75      0.75      0.74     44999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression on large-twitter-tweets dataset using word2vec vectors\n",
    "\n",
    "def tweet_to_avg_vector(tweet_tokens, w2v_vectors):\n",
    "    vectors = []\n",
    "    for word in tweet_tokens:\n",
    "        if word in w2v_vectors:  # Check if word is in the Word2Vec model\n",
    "            vectors.append(w2v_vectors[word])  # Get the word vector\n",
    "        else:\n",
    "            vectors.append(np.zeros(300))  # Use a zero vector for words not in the model\n",
    "    if vectors:  \n",
    "        return np.mean(vectors, axis=0)  # Average the word vectors to represent the whole tweet\n",
    "    else:\n",
    "        return np.zeros(300)  # Use a zero vector for words not in the model\n",
    "\n",
    "\n",
    "# Generate word2vec representations\n",
    "x_train_vectors = np.array([tweet_to_avg_vector(tweet, w2v_vectors) for tweet in train_clean_tweets])\n",
    "x_test_vectors = np.array([tweet_to_avg_vector(tweet, w2v_vectors) for tweet in test_clean_tweets])\n",
    "\n",
    "# Labels for training and testing data\n",
    "label_train = temp_train_df[\"sentiment\"]\n",
    "label_test = temp_test_df[\"sentiment\"]\n",
    "\n",
    "# Train a LOGISTIC regression model (using word2vec vectors)\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(x_train_vectors, label_train)\n",
    "\n",
    "# Get predictions then evaluate performance on test set\n",
    "label_pred = lr.predict(x_test_vectors)\n",
    "print(classification_report(label_test, label_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a90d17e8-9f00-4f60-824d-e46acb13241c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.71      0.74     18967\n",
      "           1       0.80      0.85      0.82     26032\n",
      "\n",
      "    accuracy                           0.79     44999\n",
      "   macro avg       0.79      0.78      0.78     44999\n",
      "weighted avg       0.79      0.79      0.79     44999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression on large-twitter-tweets dataset using TF-idf vectors\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Convert tokenized tweets into strings that tf-idf vectorizer can actually use\n",
    "train_tweets_clean_joined = [' '.join(tokens) for tokens in train_clean_tweets]\n",
    "test_tweets_clean_joined = [' '.join(tokens) for tokens in test_clean_tweets]\n",
    "\n",
    "train_tweets_tfidf = vectorizer.fit_transform(train_tweets_clean_joined)\n",
    "test_tweets_tfidf = vectorizer.transform(test_tweets_clean_joined)\n",
    "\n",
    "\n",
    "# Train a multinomial logistic regression model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(train_tweets_tfidf, label_train)\n",
    "\n",
    "# Get predictions then evaluate performance on test set\n",
    "label_pred = lr.predict(test_tweets_tfidf)\n",
    "print(classification_report(label_test, label_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c7976-40a9-4248-a76e-9749f58f4995",
   "metadata": {},
   "source": [
    "### TF-idf or word2vec (large-twitter-tweets-sentiment)?\n",
    "\n",
    "#### Word2vec\n",
    "There are more positive tweets (104,125 in training, 26,032 in testing) than negative tweets (75,860 in training, 18,967 in testing) in the dataset, and that's reflected in the logistic regression classifier's performance. Negative tweets had an F1 score of 0.68, while positive tweets had an F1 score of 0.79. The macro and micro averages are the same though, both sitting at 0.74.\n",
    "\n",
    "#### TF-idf\n",
    "Again, the F1 score for the negative class (0.74) is lower than the F1 score for the positive class (0.82). In this case, the macro and micro averages were very similar with a macro average of 0.78 and a micro average of 0.79.\n",
    "\n",
    "\n",
    "Comparing the performance of the two models (logstic regression trained with word2vec vs tf-idf vectors), the TF-idf-trained model performed slightly better in this case, and it's faster, so I will be sticking with the tf-idf logistic regression model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65a83298-088c-46ff-91db-5652f8abe9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/zachgendreau/.cache/kagglehub/datasets/kazanova/sentiment140/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8ad6c3-d3bd-43b0-acc6-8e4adb1472e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date      flag  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df = pd.read_csv(path + '/training.1600000.processed.noemoticon.csv', encoding='latin-1',  names=[\"sentiment\", \"id\", \"date\", \"flag\", \"user\", \"text\"])\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74538d77-d559-46f8-a1fb-01c04855cc9a",
   "metadata": {},
   "source": [
    "### Dataset Description: Twitter Sentiment140\n",
    "The Sentiment140 dataset is a dataset of Tweets that was collected as part of a research paper which can be found at this link: (https://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf).\n",
    "\n",
    "The original use of the dataset was test how machine learning models would perform when classifying the sentiment of Tweets when trained on data including emoticons like \":)\". Rather than hand-labeling data, they used the emoticons as a noisy label for the sentiment of each tweet. Tweets with emoticons like \":)\" were labeled as containing positive sentiment, while tweets with emoticons like \":(\" were labeled as negative. \n",
    "\n",
    "The Tweets were gathered through various queries using Twitter's API, and tweets containing both positive and negative emoticons were removed from the dataset. Each tweet is also recorded with data like when it was created, the user who created the tweet, the sentiment label given to the tweet, and the query used to find the tweet, if any.\n",
    "\n",
    "There are 1,600,000 Tweets in the dataset, and there are tweets from 659,775 different users. The tweets are taken from the time period of April 17th, 2009 to May 27th, 2009. The distribution of sentiments is even, with 800,000 positive tweets and 800,000 negative tweets. The average length of tweets in the corpus is 14.85 words.\n",
    "\n",
    "We will be using this dataset as the corpus we're analyzing.\n",
    "\n",
    "##### Note: Realized Kaggle tricked me because the Sentiment140 dataset page had a note that said \"target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\" but there are actually only positive / negative labels in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bd95665-f139-4106-9d17-cbdef3cd652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to convert numerical sentiment labels into text\n",
    "def map_sentiment(value):\n",
    "    if value == 4:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "twitter_df[\"sentiment_num_conv\"] = twitter_df[\"sentiment\"].apply(map_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d60c091d-50dd-4c04-9783-851ee6b72da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.77    800000\n",
      "           1       0.75      0.84      0.79    800000\n",
      "\n",
      "    accuracy                           0.78   1600000\n",
      "   macro avg       0.79      0.78      0.78   1600000\n",
      "weighted avg       0.79      0.78      0.78   1600000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize tweet text\n",
    "tweet_text = twitter_df[\"text\"].values  \n",
    "# tweet_text = corpus_sample[\"text\"].values\n",
    "tweet_tokenized = [tokenizer.tokenize(tweet) for tweet in tweet_text]\n",
    "corpus_tweets_clean = [_remove_airline_tok(tokens) for tokens in tweet_tokenized]\n",
    "\n",
    "# Convert tokenized tweets into strings that tf-idf vectorizer can actually use\n",
    "corpus_tweets_clean_joined = [' '.join(tokens) for tokens in corpus_tweets_clean]\n",
    "####\n",
    "\n",
    "# train_tweets_tfidf = vectorizer.fit_transform(train_tweets_clean_joined)\n",
    "corpus_tweets_tfidf = vectorizer.transform(corpus_tweets_clean_joined)\n",
    "\n",
    "\n",
    "# Get predictions then evaluate performance on test set\n",
    "corpus_label_pred = lr.predict(corpus_tweets_tfidf)\n",
    "print(classification_report(twitter_df[\"sentiment_num_conv\"], corpus_label_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940978c9-5006-4eb7-9e9e-f97321106622",
   "metadata": {},
   "source": [
    "#### Research Questions\n",
    "\n",
    "- What’s the relationship between sentiment and popularity?\n",
    "\n",
    "- Potential sub-question: Which entities have the most positive or negative sentiment, on average? (bert)\n",
    "\n",
    "- What are the most common entities in tweets associated with different sentiment labels?\n",
    "\n",
    "- What types of entities are the most common? Which entity types are more associated with each sentiment label? \n",
    "\n",
    "- What's the relationship between length of tweets and their average sentiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046c27fa-4230-4ca7-a983-73253ec47a99",
   "metadata": {},
   "source": [
    "### What’s the relationship between sentiment and popularity???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50f56d4a-2e80-4c80-b39c-3117fed85d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.6.0) was trained with spaCy v3.6.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[38], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m nlp_web \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m doc_web \u001b[38;5;241m=\u001b[39m twitter_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(nlp_web)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4638\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4639\u001b[0m \u001b[38;5;124;03mInvoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4640\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4755\u001b[0m \u001b[38;5;124;03mdtype: float64\u001b[39;00m\n\u001b[1;32m   4756\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4758\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4759\u001b[0m     func,\n\u001b[1;32m   4760\u001b[0m     convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4761\u001b[0m     by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4762\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4763\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4764\u001b[0m )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1290\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1291\u001b[0m )\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/spacy/language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1049\u001b[0m     doc \u001b[38;5;241m=\u001b[39m proc(doc, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomponent_cfg\u001b[38;5;241m.\u001b[39mget(name, {}))  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/spacy/pipeline/tok2vec.py:126\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39malloc((\u001b[38;5;241m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[0;32m--> 126\u001b[0m tokvecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(docs)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/thinc/model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03monly the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03mcallback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/thinc/layers/with_array.py:42\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _list_forward(model, Xseq, is_train))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/thinc/layers/with_array.py:77\u001b[0m, in \u001b[0;36m_list_forward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     76\u001b[0m Xf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(Xs, pad\u001b[38;5;241m=\u001b[39mpad)\n\u001b[0;32m---> 77\u001b[0m Yf, get_dXf \u001b[38;5;241m=\u001b[39m layer(Xf, is_train)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYs: ListXd) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ListXd:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03mcallback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03mcallback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/thinc/layers/residual.py:41\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output \u001b[38;5;241m+\u001b[39m dX\n\u001b[0;32m---> 41\u001b[0m Y, backprop_layer \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](X, is_train)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03mcallback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03mcallback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n",
      "    \u001b[0;31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m layer(X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03mcallback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_func(\u001b[38;5;28mself\u001b[39m, X, is_train\u001b[38;5;241m=\u001b[39mis_train)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/thinc/layers/maxout.py:52\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     51\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape2f(W, nO \u001b[38;5;241m*\u001b[39m nP, nI)\n\u001b[0;32m---> 52\u001b[0m Y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mgemm(X, W, trans2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape1f(b, nO \u001b[38;5;241m*\u001b[39m nP)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3574\u001b[0m, in \u001b[0;36mInteractiveShell.run_code\u001b[0;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[1;32m   3572\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   3573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3574\u001b[0m         result\u001b[38;5;241m.\u001b[39merror_in_exec \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshowtraceback(running_compiled_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Time for NER!\n",
    "\n",
    "# Load spacy models and using for NER\n",
    "import spacy\n",
    "\n",
    "# load a spacy model trained on web text\n",
    "nlp_web = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc_web = twitter_df['text'].apply(nlp_web) # Apply nlp web to each row in the ['document'] series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55e26e6d-f93f-423c-ac89-cfafae6289dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_web' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m entity_spans \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Iterate over each tweet and count spans\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m doc_web:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments:\n\u001b[1;32m      8\u001b[0m         entity_spans\u001b[38;5;241m.\u001b[39mupdate([ent\u001b[38;5;241m.\u001b[39mtext])  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'doc_web' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Initialize entity span counter\n",
    "entity_spans = Counter()\n",
    "\n",
    "# Iterate over each tweet and count spans\n",
    "for doc in doc_web:\n",
    "    for ent in doc.ents:\n",
    "        entity_spans.update([ent.text])  \n",
    "\n",
    "# Get top 10 most common spans\n",
    "most_common_ents = entity_spans.most_common(10)\n",
    "\n",
    "# Print the top 10 most common spans with their counts\n",
    "print(\"Top 10 most common entity spans:\")\n",
    "for span, count in most_common_ents:\n",
    "    print(f\"  - {span}: {count} instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3770c950-8630-4c05-a9d5-9a93144bb879",
   "metadata": {},
   "source": [
    "Based on the top 10 most common spans, there doesn't really seem to be a relationship! If a label of 0 = negative sentiment and a 1 = positive sentiment, then on average, entities that appear in more positive tweets would have a sentiment closer to 1. Lookin at the top 10 most common spans and their average sentiments, we get this:\n",
    "\n",
    "Top 10 most common entity spans and their average sentiment (logistic regression prediction):\n",
    "  - today: 57597 instances || Average sentiment = 0.46\n",
    "  - tomorrow: 27349 instances || Average sentiment = 0.45\n",
    "  - 2: 26003 instances || Average sentiment = 0.49\n",
    "  - tonight: 23124 instances || Average sentiment = 0.52\n",
    "  - one: 14757 instances || Average sentiment = 0.6\n",
    "  - first: 12105 instances || Average sentiment = 0.69\n",
    "  - 4: 9864 instances || Average sentiment = 0.55\n",
    "  - last night: 9733 instances || Average sentiment = 0.43\n",
    "  - morning: 8907 instances || Average sentiment = 0.78\n",
    "  - yesterday: 8535 instances || Average sentiment = 0.43\n",
    "\n",
    "Top 10 most common entity spans and their average sentiment (twitter_df ground truth):\n",
    "  - today: 57597 instances || Average sentiment = 0.45\n",
    "  - tomorrow: 27349 instances || Average sentiment = 0.44\n",
    "  - 2: 26003 instances || Average sentiment = 0.46\n",
    "  - tonight: 23124 instances || Average sentiment = 0.5\n",
    "  - one: 14757 instances || Average sentiment = 0.56\n",
    "  - first: 12105 instances || Average sentiment = 0.64\n",
    "  - 4: 9864 instances || Average sentiment = 0.52\n",
    "  - last night: 9733 instances || Average sentiment = 0.41\n",
    "  - morning: 8907 instances || Average sentiment = 0.72\n",
    "  - yesterday: 8535 instances || Average sentiment = 0.42\n",
    "\n",
    "It looks like, aside from \"one,\" \"first,\" and \"morning,\" the ten most common entity spans generally have average sentiment scores around 0.5 or slightly below it! It seems that an entity being more common or popular doesn't make it related to more positive or negative discussions. Another note is that the average sentiments generated from the sentiment predictions from the logistic regression model tend to be a bit higher than the averages generated from ground truth sentiment labels for the top 10 most common entities (ex: \"morning\" has an average sentiment of 0.78 according to the logistic regression labels, but a 0.72 based on ground truth sentiment labels).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caac8f6-1ed5-4816-b41c-0a46ea11be0c",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "351b52af-4528-4aee-966d-ba5fbbcbf37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "903e3357-ffa6-4f4d-b27e-4a899e89000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(temp_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb503b39-8fca-40b4-a5b0-02c9efbe8f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset.from_pandas(temp_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abdbcb51-7060-4439-8e76-4ae5f425bb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4fe69107534283b91c78cfab4ff592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/179995 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f5d801a4444efe934afa0b73ef4479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/44999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Columns ['label'] not in the dataset. Current columns in the dataset: ['sentiment', 'text', 'input_ids', 'token_type_ids', 'attention_mask']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mmap(preprocess, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Set the format for PyTorch tensors\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m train_dataset\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m'\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m test_dataset\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m'\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m func(dataset, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/arrow_dataset.py:2523\u001b[0m, in \u001b[0;36mDataset.set_format\u001b[0;34m(self, type, columns, output_all_columns, **format_kwargs)\u001b[0m\n\u001b[1;32m   2521\u001b[0m     missing_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[1;32m   2522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing_columns:\n\u001b[0;32m-> 2523\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2524\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(missing_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2525\u001b[0m         )\n\u001b[1;32m   2526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2527\u001b[0m     columns \u001b[38;5;241m=\u001b[39m columns\u001b[38;5;241m.\u001b[39mcopy()  \u001b[38;5;66;03m# Ensures modifications made to the list after this call don't cause bugs\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Columns ['label'] not in the dataset. Current columns in the dataset: ['sentiment', 'text', 'input_ids', 'token_type_ids', 'attention_mask']"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess(data):\n",
    "    return tokenizer(data['text'], padding='max_length', truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Set the format for PyTorch tensors\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64664731-dfea-4137-9bee-b11efbd96309",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_dataset[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "print(train_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5cea4d-2769-46b9-b48b-ef3dffdb62b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
